{"id": 1, "title": "Flow-Guided-Feature-Aggregation-README.md", "text": "**Flow-Guided Feature Aggregation (FGFA)** is initially described in an [ICCV 2017 paper](https://arxiv.org/abs/1703.10025). It provides an accurate and end-to-end learning framework for video object detection. The proposed FGFA method, together with our previous work of [Deep Feature Flow](https://github.com/msracver/Deep-Feature-Flow), powered the winning entry of [ImageNet VID 2017](http://image-net.org/challenges/LSVRC/2017/results). It is worth noting that:\n\n* FGFA improves the per-frame features by aggregating nearby frame features along the motion paths. It significantly improves the object detection accuracy in videos, especially for fast moving objects.\n* FGFA is end-to-end trainable for the task of video object detection, which is vital for improving the recognition accuracy.\n* We proposed to evaluate the detection accuracy for slow, medium and fast moving objects respectively, for better understanding and analysis of video object detection. The [motion-specific evaluation code](lib/dataset/imagenet_vid_eval_motion.py) is included in this repository.\n\n***Click image to watch our demo video***\n\n[![Demo Video on YouTube](https://media.giphy.com/media/7D9tmDgzB10HK/giphy.gif)](https://www.youtube.com/watch?v=R2h3DbTPvVg)\n\n***Example object instances with slow, medium and fast motions***\n\n![Instance Motion](instance_motion.png)\n\n"}
{"id": 2, "title": "LapSRN-README.md", "text": "The Laplacian Pyramid Super-Resolution Network (LapSRN) is a progressive super-resolution model that super-resolves an low-resolution images in a coarse-to-fine Laplacian pyramid framework.\nOur method is fast and achieves state-of-the-art performance on five benchmark datasets for 4x and 8x SR.\nFor more details and evaluation results, please check out our [project webpage](http://vllab.ucmerced.edu/wlai24/LapSRN/) and [paper](http://vllab.ucmerced.edu/wlai24/LapSRN/papers/cvpr17_LapSRN.pdf).\n\n![teaser](http://vllab.ucmerced.edu/wlai24/LapSRN/images/emma_text.gif)\n\n\n\n"}
{"id": 3, "title": "ICNet-README.md", "text": "Based on [PSPNet](https://github.com/hszhao/PSPNet), this repository is build for evaluation in ICNet. For installation, please follow the description in PSPNet repository (support CUDA 7.0/7.5 + cuDNN v4).\n\n"}
{"id": 4, "title": "tippecanoe-README.md", "text": " * `-n` _name_ or `--name=`_name_: Human-readable name for the tileset (default file.json)\n * `-A` _text_ or `--attribution=`_text_: Attribution (HTML) to be shown with maps that use data from this tileset.\n * `-N` _description_ or `--description=`_description_: Description for the tileset (default file.mbtiles)\n\n"}
{"id": 5, "title": "tippecanoe-README.md", "text": " * `-A` *attribution* or `--attribution=`*attribution*: Set the attribution string.\n * `-n` *name* or `--name=`*name*: Set the tileset name.\n * `-N` *description* or `--description=`*description*: Set the tileset description.\n\n"}
{"id": 6, "title": "facebookresearch-ResNeXt-README.md", "text": "This repository contains a [Torch](http://torch.ch) implementation for the [ResNeXt](https://arxiv.org/abs/1611.05431) algorithm for image classification. The code is based on [fb.resnet.torch](https://github.com/facebook/fb.resnet.torch).\n\n[ResNeXt](https://arxiv.org/abs/1611.05431) is a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call \u201ccardinality\u201d (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width.\n\n\n![teaser](http://vcl.ucsd.edu/resnext/teaser.png)\n"}
{"id": 7, "title": "mapshaper-README.md", "text": "Mapshaper is software for editing Shapefile, GeoJSON, [TopoJSON](https://github.com/mbostock/topojson/wiki), CSV and several other data formats, written in JavaScript.\n\nThe `mapshaper` command line program supports essential map making tasks like simplifying shapes, editing attribute data, clipping, erasing, dissolving, filtering and more.\n\nThe web UI supports interactive simplification, attribute data editing, and running cli commands in a built-in console. Visit the public website at [www.mapshaper.org](http://www.mapshaper.org) or use the web UI locally via the `mapshaper-gui` script.\n\nSee the [project wiki](https://github.com/mbloch/mapshaper/wiki) for more documentation on how to use mapshaper.\n\nTo suggest improvements, add an [issue](https://github.com/mbloch/mapshaper/issues).\n\nTo learn about recent updates, read the [changelog](https://github.com/mbloch/mapshaper/releases).\n\n"}
{"id": 8, "title": "RDN-README.md", "text": "A very deep convolutional neural network (CNN) has recently achieved great success for image super-resolution (SR) and offered hierarchical features as well. However, most deep CNN based SR models do not make full use of the hierarchical features from the original low-resolution (LR) images, thereby achieving relatively-low performance. In this paper, we propose a novel residual dense network (RDN) to address this problem in image SR. We fully exploit the hierarchical features from all the convolutional layers. Speci\ufb01cally, we propose residual dense block (RDB) to extract abundant local features via dense connected convolutional layers. RDB further allows direct connections from the state of preceding RDB to all the layers of current RDB, leading to a contiguous memory (CM) mechanism. Local feature fusion in RDB is then used to adaptively learn more effective features from preceding and current local features and stabilizes the training of wider network. After fully obtaining dense local features, we use global feature fusion to jointly and adaptively learn global hierarchical features in a holistic way. Experiments on benchmark datasets with different degradation models show that our RDN achieves favorable performance against state-of-the-art methods.\n\n![RDB](/Figs/RDB.png)\nFigure 1. Residual dense block (RDB) architecture.\n![RDN](/Figs/RDN.png)\nFigure 2. The architecture of our proposed residual dense network (RDN).\n\n"}
{"id": 9, "title": "DBNet-README.md", "text": "This work is based on our [research paper](http://openaccess.thecvf.com/content_cvpr_2018/html/Chen_LiDAR-Video_Driving_Dataset_CVPR_2018_paper.html), which appears in CVPR 2018. We propose a large-scale dataset for driving behavior learning, namely, DBNet. You can also check our [dataset webpage](http://www.dbehavior.net/) for a deeper introduction.\n\nIn this repository, we release __demo code__ and __partial prepared data__ for training with only images, as well as leveraging feature maps or point clouds. The prepared data are accessible [here](https://drive.google.com/open?id=14RPdVTwBTuCTo0tFeYmL_SyN8fD0g6Hc). (__More demo models and scripts are released soon!__)\n\n"}
{"id": 10, "title": "vue-README.md", "text": "Vue (pronounced `/vju\u02d0/`, like view) is a **progressive framework** for building user interfaces. It is designed from the ground up to be incrementally adoptable, and can easily scale between a library and a framework depending on different use cases. It consists of an approachable core library that focuses on the view layer only, and an ecosystem of supporting libraries that helps you tackle complexity in large Single-Page Applications.\n\n"}
{"id": 11, "title": "segyio-README.md", "text": "Segyio is a small LGPL licensed C library for easy interaction with SEG-Y and\nSeismic Unix formatted seismic data, with language bindings for Python and\nMatlab. Segyio is an attempt to create an easy-to-use, embeddable,\ncommunity-oriented library for seismic applications. Features are added as they\nare needed; suggestions and contributions of all kinds are very welcome.\n\nTo catch up on the latest development and features, see the\n[changelog](changelog.md). To write future proof code, consult the planned\n[breaking changes](breaking-changes.md).\n\n"}
{"id": 12, "title": "segyio-README.md", "text": "  * A low-level C interface with few assumptions; easy to bind to other\n    languages\n  * Read and write binary and textual headers\n  * Read and write traces and trace headers\n  * Simple, powerful, and native-feeling Python interface with numpy\n    integration\n  * Read and write seismic unix files\n  * xarray integration with netcdf_segy\n  * Some simple applications with unix philosophy\n\n"}
{"id": 13, "title": "segyio-README.md", "text": "Opening a file for reading is done with the `segyio.open` function, and\nidiomatically used with context managers. Using the `with` statement, files are\nproperly closed even in the case of exceptions. By default, files are opened\nread-only.\n\nCODE_BLOCK\n\nOpen accepts several options (for more a more comprehensive reference, check\nthe open function's docstring with `help(segyio.open)`. The most important\noption is the second (optional) positional argument. To open a file for\nwriting, do `segyio.open(filename, 'r+')`, from the C `fopen` function.\n\nFiles can be opened in *unstructured* mode, either by passing `segyio.open` the\noptional arguments `strict=False`, in which case not establishing structure\n(inline numbers, crossline numbers etc.) is not an error, and\n`ignore_geometry=True`, in which case segyio won't even try to set these\ninternal attributes.\n\nThe segy file object has several public attributes describing this structure:\n* `f.ilines`\n    Inferred inline numbers\n* `f.xlines`\n    Inferred crossline numbers\n* `f.offsets`\n    Inferred offsets numbers\n* `f.samples`\n    Inferred sample offsets (frequency and recording time delay)\n* `f.unstructured`\n    True if unstructured, False if structured\n* `f.ext_headers`\n    The number of extended textual headers\n\nIf the file is opened *unstructured*, all the line properties will will be\n`None`.\n\n"}
{"id": 14, "title": "Detectron-README.md", "text": "The goal of Detectron is to provide a high-quality, high-performance\ncodebase for object detection *research*. It is designed to be flexible in order\nto support rapid implementation and evaluation of novel research. Detectron\nincludes implementations of the following object detection algorithms:\n\n- [Mask R-CNN](https://arxiv.org/abs/1703.06870) -- *Marr Prize at ICCV 2017*\n- [RetinaNet](https://arxiv.org/abs/1708.02002) -- *Best Student Paper Award at ICCV 2017*\n- [Faster R-CNN](https://arxiv.org/abs/1506.01497)\n- [RPN](https://arxiv.org/abs/1506.01497)\n- [Fast R-CNN](https://arxiv.org/abs/1504.08083)\n- [R-FCN](https://arxiv.org/abs/1605.06409)\n\nusing the following backbone network architectures:\n\n- [ResNeXt{50,101,152}](https://arxiv.org/abs/1611.05431)\n- [ResNet{50,101,152}](https://arxiv.org/abs/1512.03385)\n- [Feature Pyramid Networks](https://arxiv.org/abs/1612.03144) (with ResNet/ResNeXt)\n- [VGG16](https://arxiv.org/abs/1409.1556)\n\nAdditional backbone architectures may be easily implemented. For more details about these models, please see [References](#references) below.\n\n"}
{"id": 15, "title": "DaSiamRPN-README.md", "text": "**SiamRPN** formulates the task of visual tracking as a task of localization and identification simultaneously, initially described in an [CVPR2018 spotlight paper](http://openaccess.thecvf.com/content_cvpr_2018/papers/Li_High_Performance_Visual_CVPR_2018_paper.pdf). (Slides at [CVPR 2018 Spotlight](https://drive.google.com/open?id=1OGIOUqANvYfZjRoQfpiDqhPQtOvPCpdq))\n\n**DaSiamRPN** improves the performances of SiamRPN by (1) introducing an effective sampling strategy to control the imbalanced sample distribution, (2) designing a novel distractor-aware module to perform incremental learning, (3) making a long-term tracking extension. [ECCV2018](https://arxiv.org/pdf/1808.06048.pdf). (Slides at [VOT-18 Real-time challenge winners talk](https://drive.google.com/open?id=1dsEI2uYHDfELK0CW2xgv7R4QdCs6lwfr))\n\n\n  \n\n\n"}
{"id": 16, "title": "integral-human-pose-README.md", "text": "**Integral Regression** is initially described in an [ECCV 2018 paper](https://arxiv.org/abs/1711.08229). ([Slides](https://jimmysuen.github.io/slides/xiaosun_integral_human_pose_regression.pptx)).\n\nWe build a [3D pose estimation system](https://arxiv.org/abs/1809.06079) based mainly on the Integral Regression, placing second in the [ECCV2018 3D Human Pose Estimation Challenge](http://vision.imar.ro/human3.6m/ranking.php). Note that, the winner [Sarandi et al.](https://arxiv.org/pdf/1809.04987.pdf) also uses the Integral Regression (or soft-argmax) with a better [augmented 3D dataset](https://github.com/isarandi/synthetic-occlusion) in their method indicating the Integral Regression is the currently state-of-the-art 3D human pose estimation method.\n\nThe Integral Regression is also known as soft-argmax. Please refer to two contemporary works ([Luvizon et al.](https://arxiv.org/abs/1710.02322) and [Nibali et al.](https://arxiv.org/abs/1801.07372)) for a better comparision and more comprehensive understanding.\n\n\n\n\n\n"}
{"id": 17, "title": "3D-ResNets-PyTorch-README.md", "text": "This is the PyTorch code for the following papers:\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Can Spatiotemporal 3D CNNs Retrace the History of 2D CNNs and ImageNet?\",  \nProceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 6546-6555, 2018.\n](http://openaccess.thecvf.com/content_cvpr_2018/html/Hara_Can_Spatiotemporal_3D_CVPR_2018_paper.html)\n\n[\nKensho Hara, Hirokatsu Kataoka, and Yutaka Satoh,  \n\"Learning Spatio-Temporal Features with 3D Residual Networks for Action Recognition\",  \nProceedings of the ICCV Workshop on Action, Gesture, and Emotion Recognition, 2017.\n](http://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w44/Hara_Learning_Spatio-Temporal_Features_ICCV_2017_paper.pdf)\n\nThis code includes training, fine-tuning and testing on Kinetics, ActivityNet, UCF-101, and HMDB-51.  \n**If you want to classify your videos or extract video features of them using our pretrained models,\nuse [this code](https://github.com/kenshohara/video-classification-3d-cnn-pytorch).**\n\n**The Torch (Lua) version of this code is available [here](https://github.com/kenshohara/3D-ResNets).**  \nNote that the Torch version only includes ResNet-18, 34, 50, 101, and 152.\n\n"}
{"id": 18, "title": "hyvr-README.md", "text": "**HyVR: Turning your geofantasy into reality!** \n\nThe Hydrogeological Virtual Reality simulation package (HyVR) is a Python module\nthat helps researchers and practitioners generate subsurface models with\nmultiple scales of heterogeneity that are based on geological concepts. The\nsimulation outputs can then be used to explore groundwater flow and solute\ntransport behaviour. This is facilitated by HyVR outputs in common flow\nsimulation packages' input formats. As each site is unique, HyVR has been\ndesigned that users can take the code and extend it to suit their particular\nsimulation needs.\n\nThe original motivation for HyVR was the lack of tools for modelling sedimentary\ndeposits that include bedding structure model outputs (i.e., dip and azimuth).\nSuch bedding parameters were required to approximate full hydraulic-conductivity\ntensors for groundwater flow modelling. HyVR is able to simulate these bedding\nparameters and generate spatially distributed parameter fields, including full\nhydraulic-conductivity tensors. More information about HyVR is available in the\nonline `technical documentation `_.\n\nI hope you enjoy using HyVR much more than I enjoyed putting it together! I look\nforward to seeing what kind of funky fields you created in the course of your\nwork.\n\n*HyVR can be attributed by citing the following journal article: Bennett, J. P., Haslauer, C. P., Ross, M., & Cirpka, O. A. (2018). An open, object-based framework for generating anisotropy in sedimentary subsurface models. Groundwater. DOI:* `10.1111/gwat.12803 `_. *A preprint version of the article is available* `here `_.\n\n"}
