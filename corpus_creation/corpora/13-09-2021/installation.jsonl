{"id": 1, "title": "LibGEOS.jl-README.md", "text": "1. At the Julia prompt, run \n  CODE_BLOCK\n  This will install both the Julia package and GEOS shared libraries together. To just reinstall the GEOS shared libraries, run `Pkg.build(\"LibGEOS\")`.\n\n2. Test that `LibGEOS` works by runnning\n  CODE_BLOCK\n"}
{"id": 2, "title": "Flow-Guided-Feature-Aggregation-README.md", "text": "1. Clone the Flow-Guided Feature Aggregation repository, and we call the directory that you cloned as ${FGFA_ROOT}.\n\n~~~\ngit clone https://github.com/msracver/Flow-Guided-Feature-Aggregation.git\n~~~\n2. For Windows users, run ``cmd .\\init.bat``. For Linux user, run `sh ./init.sh`. The scripts will build cython module automatically and create some folders.\n\n3. Install MXNet:\n\n\t3.1 Clone MXNet and checkout to [MXNet@(v0.10.0)](https://github.com/apache/incubator-mxnet/tree/v0.10.0) by\n\tCODE_BLOCK\n\t3.2 Copy operators in `$(FGFA_ROOT)/fgfa_rfcn/operator_cxx` to `$(YOUR_MXNET_FOLDER)/src/operator/contrib` by\n\tCODE_BLOCK\n\t3.3 Compile MXNet\n\tCODE_BLOCK\n\t3.4 Install the MXNet Python binding by\n\n\t***Note: If you will actively switch between different versions of MXNet, please follow 3.5 instead of 3.4***\n\tCODE_BLOCK\n\t3.5 For advanced users, you may put your Python packge into `./external/mxnet/$(YOUR_MXNET_PACKAGE)`, and modify `MXNET_VERSION` in `./experiments/fgfa_rfcn/cfgs/*.yaml` to `$(YOUR_MXNET_PACKAGE)`. Thus you can switch among different versions of MXNet quickly.\n\n\n"}
{"id": 3, "title": "Flow-Guided-Feature-Aggregation-README.md", "text": "1. Please download ILSVRC2015 DET and ILSVRC2015 VID dataset, and make sure it looks like this:\n\n\tCODE_BLOCK\n\n2. Please download ImageNet pre-trained ResNet-v1-101 model and Flying-Chairs pre-trained FlowNet model manually from [OneDrive](https://1drv.ms/u/s!Am-5JzdW2XHzhqMOBdCBiNaKbcjPrA), and put it under folder `./model`. Make sure it looks like this:\n\tCODE_BLOCK\n\n"}
{"id": 4, "title": "empymod-README.md", "text": "You can install empymod either via ``conda``:\n\n.. code-block:: console\n\n   conda install -c prisae empymod\n\nor via ``pip``:\n\n.. code-block:: console\n\n   pip install empymod\n\nRequired are Python version 3.5 or higher and the modules `NumPy` and `SciPy`.\nConsult the installation notes in the `manual\n`_ for more\ninformation regarding installation and requirements.\n\n\n"}
{"id": 5, "title": "vue-devtools-README.md", "text": "- [Get the Chrome Extension](https://chrome.google.com/webstore/detail/vuejs-devtools/nhdogjmejiglipccpnnnanhbledajbpd) / ([beta channel](https://chrome.google.com/webstore/detail/vuejs-devtools/ljjemllljcmogpfapbkkighbhhppjdbg))\n\n- [Get the Firefox Addon](https://addons.mozilla.org/en-US/firefox/addon/vue-js-devtools/) / ([beta channel](https://github.com/vuejs/vue-devtools/releases))\n\n- [Get standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)\n\n"}
{"id": 6, "title": "vue-devtools-README.md", "text": "This is only necessary when you want to build the extension yourself from source to get not-yet-released features.\n\n**Make sure you are using Node 6+ and NPM 3+**\n\n1. Clone this repo\n2. `npm install` (Or `yarn install` if you are using yarn as the package manager)\n3. `npm run build`\n4. Open Chrome extension page\n5. Check \"developer mode\"\n6. Click \"load unpacked extension\", and choose `shells/chrome`.\n\n"}
{"id": 7, "title": "vue-devtools-README.md", "text": "1. Fixing \"Download the Vue Devtools for a better development experience\" console message when working locally over `file://` protocol:\n  1.1 - Google Chrome: Right click on vue-devtools icon and click \"Manage Extensions\" then search for vue-devtools on the extensions list. Check the \"Allow access to file URLs\" box.\n\n2. How to use the devtools in IE/Edge/Safari or any other browser? [Get the standalone Electron app (works with any environment!)](https://github.com/vuejs/vue-devtools/blob/master/shells/electron/README.md)\n\n\n"}
{"id": 8, "title": "pyro-ppl-pyro-README.md", "text": "**Install using pip:**\n\nPyro supports Python 3.4+.\n\nCODE_BLOCK\n\n**Install from source:**\nCODE_BLOCK\n\n**Install with extra packages:**\n\nTo install the dependencies required to run the probabilistic models included in the `examples`/`tutorials` directories, please use the following command:\nCODE_BLOCK\nMake sure that the models come from the same release version of the [Pyro source code](https://github.com/pyro-ppl/pyro/releases) as you have installed.\n\n"}
{"id": 9, "title": "pyro-ppl-pyro-README.md", "text": "For recent features you can install Pyro from source.\n\n**Install using pip:**\n\nCODE_BLOCK\n\nor, with the `extras` dependency to run the probabilistic models included in the `examples`/`tutorials` directories:\nCODE_BLOCK\n\n**Install from source:**\n\nCODE_BLOCK\n\n"}
{"id": 10, "title": "LapSRN-README.md", "text": "Download repository:\n\n    $ git clone https://github.com/phoenix104104/LapSRN.git\n\nRun install.m in MATLAB to compile MatConvNet:\n\n    "}
{"id": 11, "title": "apsg-README.md", "text": "In rare cases, users reported problems on certain systems with the default pip installation command, which installs APSG from the binary distribution (\"wheels\") on PyPI. If you should encounter similar problems, you could try to install APSG from the source distribution instead via\nCODE_BLOCK\nAlso, I would appreciate it if you could report any issues that occur when using `pip install apsg` in hope that we can fix these in future releases.\n\n"}
{"id": 12, "title": "apsg-README.md", "text": "Installing `apsg` from the `conda-forge` channel can be achieved by adding `conda-forge` to your channels with:\n\nCODE_BLOCK\n\nOnce the `conda-forge` channel has been enabled, `apsg` can be installed with:\n\nCODE_BLOCK\n\nIt is possible to list all of the versions of `apsg` available on your platform with:\n\nCODE_BLOCK\n\n"}
{"id": 13, "title": "DCPDN-README.md", "text": "1. Install PyTorch and dependencies from http://pytorch.org (Ubuntu+Python2.7)\n   (conda install pytorch torchvision -c pytorch)\nInstall pytorch 0.3.1 https://pytorch.org/previous-versions/\n2. Install Torch vision from the source.\n   \n   \tgit clone https://github.com/pytorch/vision\n\t\n   \tcd vision\n\t\n\tpython setup.py install\n\n3. Install python package: \n   numpy, scipy, PIL, pdb\n   \n"}
{"id": 14, "title": "gempy-README.md", "text": "We provide the latest release version of *GemPy* via the **Conda** and **PyPi** package services. We highly\nrecommend using either PyPi as it will take care of automatically installing all dependencies.\n\n"}
{"id": 15, "title": "gempy-README.md", "text": "Otherwise you can clone the current repository by downloading is manually or by using Git by calling\n\n`$ git clone https://github.com/cgre-aachen/gempy.git`\n\nand then manually install it using the provided Python install file by calling\n\n`$ python gempy/setup.py install`\n\nin the cloned or downloaded repository folder. Make sure you have installed all necessary dependencies listed above before using *GemPy*.\n\n"}
{"id": 16, "title": "gempy-README.md", "text": "1) Install CUDA if you do not have it already.\n\n2) Install Anaconda3 2019.03 with Python 3.7 (this is the last release).\n\n3) Install Theano and associated packages from the Anaconda prompt as administrator, and finally install GemPy 2.0:\n\n- conda update --all\n- conda install libpython\n- conda install m2w64-toolchain\n- conda install git\n- conda install pygpu\n- pip install theano==1.0.4\n- pip install gempy==2.0b0.dev2\n\nNote that:\n\na) some other packages required by Theano are already included in Anaconda: numpy, scipy, mkl-service, nose, and sphinx.\n\nb) pydot-ng (suggested on Theano web site) yields a lot of errors. I dropped this. It is needed to handle large picture for gif/images and probably it is not needed by GemPy.\n\nc) Trying to install all the packages in one go but it does not work, as well as doing the same in Anaconda Navigator, or installing an older Anaconda release with Python 3.5 (Anaconda3 4.2.0) as indicated in some tutorial on Theano.\n\n\n\n"}
{"id": 17, "title": "Shapely-README.md", "text": "Shapely may be installed from a source distribution or one of several kinds\nof built distribution.\n\n"}
{"id": 18, "title": "scikit-image-scikit-image-README.md", "text": "- **Debian/Ubuntu:** ``sudo apt-get install python-skimage``\n- **OSX:** ``pip install scikit-image``\n- **Anaconda:** ``conda install -c conda-forge scikit-image``\n- **Windows:** Download [Windows binaries](http://www.lfd.uci.edu/~gohlke/pythonlibs/#scikit-image)\n\nAlso see [installing ``scikit-image``](INSTALL.rst).\n\n"}
{"id": 19, "title": "scikit-image-scikit-image-README.md", "text": "Install dependencies using:\n\nCODE_BLOCK\n\nThen, install scikit-image using:\n\nCODE_BLOCK\n\nIf you plan to develop the package, you may run it directly from source:\n\nCODE_BLOCK\n\nEvery time you modify Cython files, also run:\n\nCODE_BLOCK\n\n"}
{"id": 20, "title": "pyGeoPressure-README.md", "text": "`pyGeoPressure` is on `PyPI`:\n\nCODE_BLOCK\n\n"}
{"id": 21, "title": "sg2im-README.md", "text": "All code was developed and tested on Ubuntu 16.04 with Python 3.5 and PyTorch 0.4.\n\nYou can setup a virtual environment to run the code like this:\n\nCODE_BLOCK\n\n"}
{"id": 22, "title": "pyvista-README.md", "text": "PyVista can be installed from `PyPI `_\nusing ``pip`` on Python >= 3.5::\n\n    pip install pyvista\n\nYou can also visit `PyPi `_,\n`Anaconda `_, or\n`GitHub `_ to download the source.\n\nSee the `Installation `_\nfor more details if the installation through pip doesn't work out.\n\n"}
{"id": 23, "title": "neural_renderer-README.md", "text": "CODE_BLOCK\n\n"}
{"id": 24, "title": "harismuneer-Ultimate-Facebook-Scraper-README.md", "text": "You will need to install latest version of [Google Chrome](https://www.google.com/chrome/). Moreover, you need to install selenium module as well using\n\nCODE_BLOCK\n\nRun the code using Python 3. Also, the code is multi-platform and is tested on both Windows and Linux.\nThe tool uses latest version of [Chrome Web Driver](http://chromedriver.chromium.org/downloads). I have placed the webdriver along with the code but if that version doesn't work then replace the chrome web driver with the latest one.\n\n"}
{"id": 25, "title": "geonotebook-README.md", "text": "CODE_BLOCK\n\n*Note* The `serverextension` and `nbextension` commands accept flags that configure how\nand where the extensions are installed.  See `jupyter serverextension --help` for more\ninformation.\n\n"}
{"id": 26, "title": "geonotebook-README.md", "text": "When developing geonotebook, it is often helpful to install packages as a reference to the\nchecked out repository rather than copying them to the system `site-packages`.  A \"development\ninstall\" will allow you to make live changes to python or javascript without reinstalling the\npackage.\nCODE_BLOCK\n\n"}
{"id": 27, "title": "gprMax-README.md", "text": "The following steps provide guidance on how to install gprMax:\n\n1. Install Python, required Python packages, and get the gprMax source code from GitHub\n2. Install a C compiler which supports OpenMP\n3. Build and install gprMax\n\nYou can `watch screencasts `_ that demonstrate the installation and update processes.\n\n"}
{"id": 28, "title": "gprMax-README.md", "text": "We recommend using Miniconda to install Python and the required Python packages for gprMax in a self-contained Python environment. Miniconda is a mini version of Anaconda which is a completely free Python distribution (including for commercial use and redistribution). It includes more than 300 of the most popular Python packages for science, math, engineering, and data analysis.\n\n* `Download and install Miniconda `_. Choose the Python 3.x version for your platform. We recommend choosing the installation options to: install Miniconda only for your user account; add Miniconda to your PATH environment variable; and to register Miniconda Python as your default Python. See the `Quick Install page `_ for help installing Miniconda.\n* Open a Terminal (Linux/macOS) or Command Prompt (Windows) and run the following commands:\n\n.. code-block:: bash\n\n    $ conda update conda\n    $ conda install git\n    $ git clone https://github.com/gprMax/gprMax.git\n    $ cd gprMax\n    $ conda env create -f conda_env.yml\n\nThis will make sure conda is up-to-date, install Git, get the latest gprMax source code from GitHub, and create an environment for gprMax with all the necessary Python packages.\n\nIf you prefer to install Python and the required Python packages manually, i.e. without using Anaconda/Miniconda, look in the ``conda_env.yml`` file for a list of the requirements.\n\n"}
{"id": 29, "title": "gprMax-README.md", "text": "Linux\n^^^^^\n\n* `gcc `_ should be already installed, so no action is required.\n\n\nmacOS\n^^^^^\n\n* Xcode (the IDE for macOS) comes with the LLVM (clang) compiler, but it does not currently support OpenMP, so you must install `gcc `_. That said, it is still useful to have Xcode (with command line tools) installed. It can be downloaded from the App Store. Once Xcode is installed, download and install the `Homebrew package manager `_ and then to install gcc, run:\n\n.. code-block:: bash\n\n    $ brew install gcc\n\nMicrosoft Windows\n^^^^^^^^^^^^^^^^^\n\n* Download and install `Microsoft Visual C++ 2015 Build Tools `_ (currently you must use the 2015 version, not 2017). Use the custom installation option and deselect everything apart from the Windows SDK for your version of Windows.\n\nAlternatively if you are using Windows 10 and feeling adventurous you can install the `Windows Subsystem for Linux `_ and then follow the Linux install instructions for gprMax. Note however that currently WSL does not aim to support GUI desktops or applications, e.g. Gnome, KDE, etc....\n\n\n\n"}
{"id": 30, "title": "gprMax-README.md", "text": "Once you have installed the aforementioned tools follow these steps to build and install gprMax:\n\n* Open a Terminal (Linux/macOS) or Command Prompt (Windows), navigate into the top-level gprMax directory, and if it is not already active, activate the gprMax conda environment :code:`conda activate gprMax`. Run the following commands:\n\n.. code-block:: bash\n\n    (gprMax)$ python setup.py build\n    (gprMax)$ python setup.py install\n\n**You are now ready to proceed to running gprMax.**\n\nIf you have problems with building gprMax on Microsoft Windows, you may need to add :code:`C:\\Program Files (x86)\\Microsoft Visual Studio 14.0\\VC\\bin` to your path environment variable.\n\n"}
{"id": 31, "title": "kosmtik-README.md", "text": "Note: Node.js versions are moving very fast, and kosmtik or its dependencies are\nhardly totally up to date with latest release. Ideally, you should run the LTS\nversion of Node.js. You can use a Node.js version manager (like\n[NVM](https://github.com/creationix/nvm)) to help.\n\n    npm -g install kosmtik\n\nThis might need root/Administrator rights. If you cannot install globally\nyou can also install locally with\n\n    npm install kosmtik\n\nThis will create a `node_modules/kosmtik` folder. You then have to replace all occurences of `kosmtik`\nbelow with `node node_modules/kosmtik/index.js`.\n\nTo reinstall all plugins:\n\n    kosmtik plugins --reinstall\n\n"}
{"id": 32, "title": "tilematrix-README.md", "text": "Use ``pip`` to install the latest stable version:\n\n.. code-block:: shell\n\n    pip install tilematrix\n\nManually install the latest development version\n\n.. code-block:: shell\n\n    pip install -r requirements.txt\n    python setup.py install\n\n\n-------------\n"}
{"id": 33, "title": "gitfolio-README.md", "text": "Install gitfolio\n\nCODE_BLOCK\n\n"}
{"id": 34, "title": "two-stream-dyntex-synth-README.md", "text": "1. Store the appearance-stream tfmodel in `./models`.\n2. Store the dynamics-stream tfmodel in `./models`. The filepath to this model is your `--dynamics_model` path.\n\n"}
{"id": 35, "title": "PRM-README.md", "text": "1. Install [Nest](https://github.com/ZhouYanzhao/Nest), a flexible tool for building and sharing deep learning modules:\n    \n    > I created Nest in the process of refactoring PRM's pytorch implementation. It aims at encouraging code reuse and ships with a bunch of useful features. PRM is now implemented as a set of Nest modules; thus you can easily install and use it as demonstrated below.\n\n    CODE_BLOCK\n    \n\n2. Install PRM via Nest's CLI tool:\n\n    CODE_BLOCK\n\n"}
{"id": 36, "title": "tippecanoe-README.md", "text": " * `-r` _rate_ or `--drop-rate=`_rate_: Rate at which dots are dropped at zoom levels below basezoom (default 2.5).\n   If you use `-rg`, it will guess a drop rate that will keep at most 50,000 features in the densest tile.\n   You can also specify a marker-width with `-rg`*width* to allow fewer features in the densest tile to\n   compensate for the larger marker, or `-rf`*number* to allow at most *number* features in the densest tile.\n * `-B` _zoom_ or `--base-zoom=`_zoom_: Base zoom, the level at and above which all points are included in the tiles (default maxzoom).\n   If you use `-Bg`, it will guess a zoom level that will keep at most 50,000 features in the densest tile.\n   You can also specify a marker-width with `-Bg`*width* to allow fewer features in the densest tile to\n   compensate for the larger marker, or `-Bf`*number* to allow at most *number* features in the densest tile.\n * `-al` or `--drop-lines`: Let \"dot\" dropping at lower zooms apply to lines too\n * `-ap` or `--drop-polygons`: Let \"dot\" dropping at lower zooms apply to polygons too\n * `-K` _distance_ or `--cluster-distance=`_distance_: Cluster points (as with `--cluster-densest-as-needed`, but without the experimental discovery process) that are approximately within _distance_ of each other. The units are tile coordinates within a nominally 256-pixel tile, so the maximum value of 255 allows only one feature per tile. Values around 10 are probably appropriate for typical marker sizes. See `--cluster-densest-as-needed` below for behavior.\n\n"}
{"id": 37, "title": "tippecanoe-README.md", "text": " * `-M` _bytes_ or `--maximum-tile-bytes=`_bytes_: Use the specified number of _bytes_ as the maximum compressed tile size instead of 500K.\n * `-O` _features_ or `--maximum-tile-features=`_features_: Use the specified number of _features_ as the maximum in a tile instead of 200,000.\n * `-pf` or `--no-feature-limit`: Don't limit tiles to 200,000 features\n * `-pk` or `--no-tile-size-limit`: Don't limit tiles to 500K bytes\n * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.\n * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.\n * `--tile-stats-attributes-limit=`*count*: Include `tilestats` information about at most *count* attributes instead of the default 1000.\n * `--tile-stats-sample-values-limit=`*count*: Calculate `tilestats` attribute statistics based on *count* values instead of the default 1000.\n * `--tile-stats-values-limit=`*count*: Report *count* unique attribute values in `tilestats` instead of the default 100.\n\n"}
{"id": 38, "title": "tippecanoe-README.md", "text": " * `-pk` or `--no-tile-size-limit`: Don't skip tiles larger than 500K.\n * `-pC` or `--no-tile-compression`: Don't compress the PBF vector tile data.\n * `-pg` or `--no-tile-stats`: Don't generate the `tilestats` row in the tileset metadata. Uploads without [tilestats](https://github.com/mapbox/mapbox-geostats) will take longer to process.\n\nBecause tile-join just copies the geometries to the new .mbtiles without processing them\n(except to rescale the extents if necessary),\nit doesn't have any of tippecanoe's recourses if the new tiles are bigger than the 500K tile limit.\nIf a tile is too big and you haven't specified `-pk`, it is just left out of the new tileset.\n\nExample\n-------\n\nImagine you have a tileset of census blocks:\n\nCODE_BLOCK\n\nand a CSV of their populations:\n\nCODE_BLOCK\n\nwhich looks like this:\n\nCODE_BLOCK\n\nThen you can join those populations to the geometries and discard the no-longer-needed ID field:\n\nCODE_BLOCK\n\ntippecanoe-enumerate\n====================\n\nThe `tippecanoe-enumerate` utility lists the tiles that an `mbtiles` file defines.\nEach line of the output lists the name of the `mbtiles` file and the zoom, x, and y\ncoordinates of one of the tiles. It does basically the same thing as\n\n    select zoom_level, tile_column, (1 << zoom_level) - 1 - tile_row from tiles;\n\non the file in sqlite3.\n\ntippecanoe-decode\n=================\n\nThe `tippecanoe-decode` utility turns vector mbtiles back to GeoJSON. You can use it either\non an entire file:\n\n    tippecanoe-decode file.mbtiles\n\nor on an individual tile:\n\n    tippecanoe-decode file.mbtiles zoom x y\n    tippecanoe-decode file.vector.pbf zoom x y\n\nUnless you use `-c`, the output is a set of nested FeatureCollections identifying each\ntile and layer separately. Note that the same features generally appear at all zooms,\nso the output for the file will have many copies of the same features at different\nresolutions.\n\n"}
{"id": 39, "title": "reduxjs-react-redux-README.md", "text": "React Redux requires **React 16.8.3 or later.**\n\nCODE_BLOCK\n\nThis assumes that you\u2019re using [npm](http://npmjs.com/) package manager \nwith a module bundler like [Webpack](https://webpack.js.org/) or \n[Browserify](http://browserify.org/) to consume [CommonJS \nmodules](https://webpack.js.org/api/module-methods/#commonjs).\n\nIf you don\u2019t yet use [npm](http://npmjs.com/) or a modern module bundler, and would rather prefer a single-file [UMD](https://github.com/umdjs/umd) build that makes `ReactRedux` available as a global object, you can grab a pre-built version from [cdnjs](https://cdnjs.com/libraries/react-redux). We *don\u2019t* recommend this approach for any serious application, as most of the libraries complementary to Redux are only available on [npm](http://npmjs.com/).\n\n"}
{"id": 40, "title": "pysal-README.md", "text": "PySAL is available through\n[Anaconda](https://www.continuum.io/downloads) (in the defaults or\nconda-forge channel) and [Enthought\nCanopy](https://www.enthought.com/products/canopy/). We recommend\ninstalling PySAL from conda-forge:\n\nCODE_BLOCK\n\nPySAL can be installed using pip:\n\nCODE_BLOCK\n\nAs of version 2.0.0 PySAL has shifted to Python 3 only.\n\nUsers who need an older stable version of PySAL that is Python 2\ncompatible can install version 1.14.3 through pip or conda:\n\nCODE_BLOCK\n\n"}
{"id": 41, "title": "vid2vid-README.md", "text": "- Install python libraries [dominate](https://github.com/Knio/dominate) and requests.\nCODE_BLOCK\n- If you plan to train with face datasets, please install dlib.\nCODE_BLOCK\n- If you plan to train with pose datasets, please install [DensePose](https://github.com/facebookresearch/DensePose) and/or [OpenPose](https://github.com/CMU-Perceptual-Computing-Lab/openpose).\n- Clone this repo:\nCODE_BLOCK\n- Docker Image\nIf you have difficulty building the repo, a docker image can be found in the `docker` folder.\n\n"}
{"id": 42, "title": "scikit-learn-scikit-learn-README.md", "text": "Dependencies\n~~~~~~~~~~~~\n\nscikit-learn requires:\n\n- Python (>= 3.5)\n- NumPy (>= 1.11.0)\n- SciPy (>= 0.17.0)\n- joblib (>= 0.11)\n\n**Scikit-learn 0.20 was the last version to support Python 2.7 and Python 3.4.**\nscikit-learn 0.21 and later require Python 3.5 or newer.\n\nScikit-learn plotting capabilities (i.e., functions start with \"plot_\") require\nMatplotlib (>= 1.5.1). For running the examples Matplotlib >= 1.5.1 is\nrequired. A few examples require scikit-image >= 0.12.3, a few examples require\npandas >= 0.18.0.\n\nUser installation\n~~~~~~~~~~~~~~~~~\n\nIf you already have a working installation of numpy and scipy,\nthe easiest way to install scikit-learn is using ``pip``   ::\n\n    pip install -U scikit-learn\n\nor ``conda``::\n\n    conda install scikit-learn\n\nThe documentation includes more detailed `installation instructions `_.\n\n\n"}
{"id": 43, "title": "mplstereonet-README.md", "text": "``mplstereonet`` can be installed from PyPi using ``pip`` by::\n\n    pip install mplstereonet\n\nAlternatively, you can download the source and install locally using (from the\nmain directory of the repository)::\n\n    python setup.py install\n\nIf you're planning on developing ``mplstereonet`` or would like to experiment\nwith making local changes, consider setting up a development installation so\nthat your changes are reflected when you import the package::\n\n    python setup.py develop\n\n"}
{"id": 44, "title": "RDN-README.md", "text": "1. Download DIV2K training data (800 training + 100 validtion images) from [DIV2K dataset](https://data.vision.ee.ethz.ch/cvl/DIV2K/) or [SNU_CVLab](https://cv.snu.ac.kr/research/EDSR/DIV2K.tar).\n\n2. Place all the HR images in 'Prepare_TrainData/DIV2K/DIV2K_HR'.\n\n3. Run 'Prepare_TrainData_HR_LR_BI/BD/DN.m' in matlab to generate LR images for BI, BD, and DN models respectively.\n\n4. Run 'th png_to_t7.lua' to convert each .png image to .t7 file in new folder 'DIV2K_decoded'.\n\n5. Specify the path of 'DIV2K_decoded' to '-datadir' in 'RDN_TrainCode/code/opts.lua'.\n\nFor more informaiton, please refer to [EDSR(Torch)](https://github.com/LimBee/NTIRE2017).\n\n"}
{"id": 45, "title": "rasterio-README.md", "text": "    "}
{"id": 46, "title": "tilelive-mapnik-README.md", "text": "    npm install tilelive-mapnik\n\nThough `tilelive` is not a dependency of `tilelive-mapnik` you will want to\ninstall it to actually make use of `tilelive-mapnik` through a reasonable\nAPI.\n\n\n"}
{"id": 47, "title": "neural-motifs-README.md", "text": "0. Install python3.6 and pytorch 3. I recommend the [Anaconda distribution](https://repo.continuum.io/archive/). To install PyTorch if you haven't already, use\n CODE_BLOCK.\n \n1. Update the config file with the dataset paths. Specifically:\n    - Visual Genome (the VG_100K folder, image_data.json, VG-SGG.h5, and VG-SGG-dicts.json). See data/stanford_filtered/README.md for the steps I used to download these.\n    - You'll also need to fix your PYTHONPATH: CODE_BLOCK \n\n2. Compile everything. run CODE_BLOCK in the main directory: this compiles the Bilinear Interpolation operation for the RoIs as well as the Highway LSTM.\n\n3. Pretrain VG detection. The old version involved pretraining COCO as well, but we got rid of that for simplicity. Run ./scripts/pretrain_detector.sh\nNote: You might have to modify the learning rate and batch size, particularly if you don't have 3 Titan X GPUs (which is what I used). [You can also download the pretrained detector checkpoint here.](https://drive.google.com/open?id=11zKRr2OF5oclFL47kjFYBOxScotQzArX)\n\n4. Train VG scene graph classification: run ./scripts/train_models_sgcls.sh 2 (will run on GPU 2). OR, download the MotifNet-cls checkpoint here: [Motifnet-SGCls/PredCls](https://drive.google.com/open?id=12qziGKYjFD3LAnoy4zDT3bcg5QLC0qN6).\n5. Refine for detection: run ./scripts/refine_for_detection.sh 2 or download the [Motifnet-SGDet](https://drive.google.com/open?id=1thd_5uSamJQaXAPVGVOUZGAOfGCYZYmb) checkpoint.\n6. Evaluate: Refer to the scripts ./scripts/eval_models_sg[cls/det].sh.\n\n"}
{"id": 48, "title": "mplleaflet-README.md", "text": "Install `mplleaflet` from PyPI using `$ pip install mplleaflet`.\n\n"}
{"id": 49, "title": "pose-residual-network-pytorch-README.md", "text": "1. Clone this repository \n`git clone https://github.com/salihkaragoz/pose-residual-network-pytorch.git`\n\n2. Install [Pytorch](https://pytorch.org/)\n\n3. `pip install -r src/requirements.txt`\n\n4. To download COCO dataset train2017 and val2017 annotations run: `bash data/coco.sh`. (data size: ~240Mb)\n\n"}
{"id": 50, "title": "react-README.md", "text": "React has been designed for gradual adoption from the start, and **you can use as little or as much React as you need**:\n\n* Use [Online Playgrounds](https://reactjs.org/docs/getting-started.html#online-playgrounds) to get a taste of React.\n* [Add React to a Website](https://reactjs.org/docs/add-react-to-a-website.html) as a `` tag in one minute.\n* [Create a New React App](https://reactjs.org/docs/create-a-new-react-app.html) if you're looking for a powerful JavaScript toolchain.\n\nYou can use React as a `` tag from a [CDN](https://reactjs.org/docs/cdn-links.html), or as a `react` package on [npm](https://www.npmjs.com/).\n\n"}
{"id": 51, "title": "tensorflow-magenta-README.md", "text": "Magenta maintains a [pip package](https://pypi.python.org/pypi/magenta) for easy\ninstallation. We recommend using Anaconda to install it, but it can work in any\nstandard Python environment. We support both Python 2 (>= 2.7) and Python 3 (>= 3.5).\nThese instructions will assume you are using Anaconda.\n\nNote that if you want to enable GPU support, you should follow the [GPU Installation](#gpu-installation) instructions below.\n\n"}
{"id": 52, "title": "tensorflow-magenta-README.md", "text": "If you are running Mac OS X or Ubuntu, you can try using our automated\ninstallation script. Just paste the following command into your terminal.\n\nCODE_BLOCK\n\nAfter the script completes, open a new terminal window so the environment\nvariable changes take effect.\n\nThe Magenta libraries are now available for use within Python programs and\nJupyter notebooks, and the Magenta scripts are installed in your path!\n\nNote that you will need to run `source activate magenta` to use Magenta every\ntime you open a new terminal window.\n\n"}
{"id": 53, "title": "tensorflow-magenta-README.md", "text": "If the automated script fails for any reason, or you'd prefer to install by\nhand, do the following steps.\n\nInstall the Magenta pip package:\n\nCODE_BLOCK\n\n**NOTE**: In order to install the `rtmidi` package that we depend on, you may need to install headers for some sound libraries. On Linux, this command should install the necessary packages:\n\nCODE_BLOCK\n\nThe Magenta libraries are now available for use within Python programs and\nJupyter notebooks, and the Magenta scripts are installed in your path!\n\n"}
{"id": 54, "title": "tensorflow-magenta-README.md", "text": "If you have a GPU installed and you want Magenta to use it, you will need to\nfollow the [Manual Install](#manual-install) instructions, but with a few\nmodifications.\n\nFirst, make sure your system meets the [requirements to run tensorflow with GPU support](\nhttps://www.tensorflow.org/install/install_linux#nvidia_requirements_to_run_tensorflow_with_gpu_support).\n\nNext, follow the [Manual Install](#manual-install) instructions, but install the\n`magenta-gpu` package instead of the `magenta` package:\n\nCODE_BLOCK\n\nThe only difference between the two packages is that `magenta-gpu` depends on\n`tensorflow-gpu` instead of `tensorflow`.\n\nMagenta should now have access to your GPU.\n\n"}
{"id": 55, "title": "sequelize-sequelize-README.md", "text": "CODE_BLOCK\n\n"}
{"id": 56, "title": "DID-MDN-README.md", "text": "1. Install PyTorch and dependencies from http://pytorch.org (Ubuntu+Python2.7)\n   (conda install pytorch torchvision -c pytorch)\n\n2. Install Torch vision from the source.\n   (git clone https://github.com/pytorch/vision\n   cd vision\n   python setup.py install)\n\n3. Install python package: \n   numpy, scipy, PIL, pdb\n   \n"}
{"id": 57, "title": "cltk-cltk-README.md", "text": "CLTK supports Python versions 3.6 and 3.7. The software only runs on POSIX\u2013compliant operating systems (Linux, Mac OS X, FreeBSD, etc.).\n\nCODE_BLOCK\n\nSee docs for [complete installation instructions](http://docs.cltk.org/en/latest/installation.html).\n\nThe [CLTK organization curates corpora](https://github.com/cltk) which can be downloaded directly or, better, [imported by the toolkit](http://docs.cltk.org/en/latest/importing_corpora.html).\n\n\n"}
{"id": 58, "title": "ipyleaflet-README.md", "text": "Using conda:\n\nCODE_BLOCK\n\nUsing pip:\n\nCODE_BLOCK\n\nIf you have JupyterLab, you will also need to install the JupyterLab extension:\n\nCODE_BLOCK\n\nSome users have found that the ``jupyterlab-manager`` is also required\nin jupyterlab if the map does not display.\n\nCODE_BLOCK\n\n"}
{"id": 59, "title": "ipyleaflet-README.md", "text": "For a development installation (requires npm):\n\nCODE_BLOCK\n\nNote for developers:\n\n- the ``-e`` pip option allows one to modify the Python code in-place. Restart the kernel in order to see the changes.\n- the ``--symlink`` argument on Linux or OS X allows one to modify the JavaScript code in-place. This feature is not available with Windows.\n\n    For automatically building the JavaScript code every time there is a change, run the following command from the ``ipyleaflet/js/`` directory:\n\n    CODE_BLOCK\n\n    If you are on JupyterLab you also need to run the following in a separate terminal:\n\n    CODE_BLOCK\n\n    Every time a JavaScript build has terminated you need to refresh the Notebook page in order to load the JavaScript code again.\n\n"}
{"id": 60, "title": "readgssi-README.md", "text": "Once you have [anaconda](https://www.anaconda.com/download) running, installing requirements is pretty easy.\n\nCODE_BLOCK\n\nThat should allow you to run the commands below.\n\n"}
{"id": 61, "title": "readgssi-README.md", "text": "If you choose to install a specific commit rather than the [latest working release of this software](https://pypi.org/project/readgssi), you may download this package, unzip to your home folder, open a command line, then install in the following way:\n\nCODE_BLOCK\n\n"}
{"id": 62, "title": "Detectron-README.md", "text": "Please find installation instructions for Caffe2 and Detectron in [`INSTALL.md`](INSTALL.md).\n\n"}
{"id": 63, "title": "facebookresearch-pyrobot-README.md", "text": "* Install **Ubuntu 16.04**\n\n* Download the installation script\nCODE_BLOCK\n\n* Run the script to install everything (ROS, realsense driver, etc.). **Please connect the nuc machine to a realsense camera before running the following commands**.\nCODE_BLOCK\n\n"}
{"id": 64, "title": "facebookresearch-pyrobot-README.md", "text": "* Install **Ubuntu 16.04** \n\n* Install [ROS kinetic](http://wiki.ros.org/kinetic/Installation/Ubuntu)\n\n* Install KDL\n\nCODE_BLOCK\n\n* Install Python virtual environment\n\nCODE_BLOCK\n\n* Install PyRobot \n\nCODE_BLOCK\n\n**Warning**: As realsense keeps updating, compatibility issues might occur if you accidentally update \nrealsense-related packages from `Software Updater` in ubuntu. Therefore, we recommend you not to update\nany libraries related to realsense. Check the list of updates carefully when ubuntu prompts software udpates.\n\n"}
{"id": 65, "title": "facebookresearch-DensePose-README.md", "text": "Please find installation instructions for Caffe2 and DensePose in [`INSTALL.md`](INSTALL.md), a document based on the [Detectron](https://github.com/facebookresearch/Detectron) installation instructions.\n\n"}
{"id": 66, "title": "DaSiamRPN-README.md", "text": "- install pytorch, numpy, opencv following the instructions in the `run_install.sh`. Please do **not** use conda to install.\n- you can alternatively modify `/PATH/TO/CODE/FOLDER/` in `tracker_SiamRPN.m` \n  If the tracker is ready, you will see the tracking results. (EAO: 0.3827)\n\n\n"}
{"id": 67, "title": "hmr-README.md", "text": "CODE_BLOCK\n"}
{"id": 68, "title": "hmr-README.md", "text": "With GPU:\nCODE_BLOCK\nWithout GPU:\nCODE_BLOCK\n\n"}
{"id": 69, "title": "hmr-README.md", "text": "This is only partialy tested.\nCODE_BLOCK\n"}
{"id": 70, "title": "GPRPy-README.md", "text": "**In the following instructions, if you use Windows, use the comands `python` and `pip`. If you use Mac or Linux, use the commands `python3` and `pip3` instead.**\n\n1) Download the GPRPy software from \n   [https://github.com/NSGeophysics/GPRPy/archive/master.zip](https://github.com/NSGeophysics/GPRPy/archive/master.zip). \n   Save the file somewhere on your computer and extract the zip folder. \n   As an **alternative**, you can install git from [https://git-scm.com/](https://git-scm.com/), then run in a command prompt:\n   `git clone https://github.com/NSGeophysics/GPRPy.git`\n   The advantage of the latter is that you can easily update your software by running from the GPRPy folder in a command prompt:\n   `git pull origin master`\n\n2) Install Python 3.7 for example from [https://conda.io/miniconda.html](https://conda.io/miniconda.html)\n\n3) Once the installation finished, open a command prompt that can run Python \n   On Windows: click on Start, then enter \"Anaconda Prompt\", without the quotation marks into the \"Search programs and files\" field. On Mac or Linux, open the regular terminal.\n\n4) In the command prompt, change to the directory  where you downloaded the GPRPy files.\n   This is usually through a command like for example\n   `cd Desktop\\GPRPy`\n   if you downloaded GPRPy directly onto your desktop. Then type the following and press enter afterward:\n   `python installMigration.py`\n   Then type the following and press enter afterward:\n   `pip install .`\n   **don't forget the period \".\" at the end of the `pip install` command**\n\n\n"}
{"id": 71, "title": "d3-README.md", "text": "If you use npm, `npm install d3`. Otherwise, download the [latest release](https://github.com/d3/d3/releases/latest). The released bundle supports anonymous AMD, CommonJS, and vanilla environments. You can load directly from [d3js.org](https://d3js.org), [CDNJS](https://cdnjs.com/libraries/d3), or [unpkg](https://unpkg.com/d3/). For example:\n\nCODE_BLOCK\n\nFor the minified version:\n\nCODE_BLOCK\n\nYou can also use the standalone D3 microlibraries. For example, [d3-selection](https://github.com/d3/d3-selection):\n\nCODE_BLOCK\n\nD3 is written using [ES2015 modules](http://www.2ality.com/2014/09/es6-modules-final.html). Create a [custom bundle using Rollup](https://bl.ocks.org/mbostock/bb09af4c39c79cffcde4), Webpack, or your preferred bundler. To import D3 into an ES2015 application, either import specific symbols from specific D3 modules:\n\nCODE_BLOCK\n\nOr import everything into a namespace (here, `d3`):\n\nCODE_BLOCK\n\nIn Node:\n\nCODE_BLOCK\n\nYou can also require individual modules and combine them into a `d3` object using [Object.assign](https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/Object/assign):\n\nCODE_BLOCK\n"}
{"id": 72, "title": "iter-reason-README.md", "text": "1. Clone the repository.\n  CODE_BLOCK\n\n2. Set up data, here we use [ADE20K](http://groups.csail.mit.edu/vision/datasets/ADE20K/) as an example.\n  CODE_BLOCK\n\n3. Set up pre-trained ImageNet models. This is similarly done in [tf-faster-rcnn](https://github.com/endernewton/tf-faster-rcnn). Here by default we use ResNet-50 as the backbone:\n  CODE_BLOCK\n\n4. Compile the library (for computing bounding box overlaps).\n  CODE_BLOCK\n\n5. Now you are ready to run! For example, to train and test the baseline:\n  CODE_BLOCK\n\n6. To train and test the reasoning modules (based on ResNet-50):\n  CODE_BLOCK\n\n7. Once the training is done, you can test the models separately with `test.sh` and `test_memory.sh`, we also provided a separate set of scripts to test on larger image inputs.\n\n8. You can use tensorboard to visualize and track the progress, for example:\n  CODE_BLOCK\n\n"}
{"id": 73, "title": "pylops-README.md", "text": "To ensure that further development of PyLops is performed within the same environment (i.e., same dependencies) as\nthat defined by ``requirements-dev.txt`` or ``environment-dev.yml`` files, we suggest to work off a new Conda enviroment.\n\nThe first time you clone the repository run the following command:\nCODE_BLOCK\nTo ensure that everything has been setup correctly, run tests:\nCODE_BLOCK\nMake sure no tests fail, this guarantees that the installation has been successfull.\n\nRemember to always activate the conda environment every time you open a new terminal by typing:\nCODE_BLOCK\n\n"}
{"id": 74, "title": "puppeteer-README.md", "text": "To use Puppeteer in your project, run:\n\nCODE_BLOCK\n\nNote: When you install Puppeteer, it downloads a recent version of Chromium (~170MB Mac, ~282MB Linux, ~280MB Win) that is guaranteed to work with the API. To skip the download, see [Environment variables](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#environment-variables).\n\n\n"}
{"id": 75, "title": "puppeteer-README.md", "text": "We have a [troubleshooting](https://github.com/GoogleChrome/puppeteer/blob/master/docs/troubleshooting.md) guide for various operating systems that lists the required dependencies.\n\n"}
{"id": 76, "title": "integral-human-pose-README.md", "text": "We recommend installing python from [Anaconda](https://www.anaconda.com/), installing pytorch following guide on [PyTorch](https://pytorch.org/) according to your specific CUDA & python version.\nIn addition, you need to install dependencies below.\nCODE_BLOCK \n\n\n"}
{"id": 77, "title": "integral-human-pose-README.md", "text": "1. Download Human3.6M(ECCV18 Challenge) image from [Human3.6M Dataset](http://vision.imar.ro/human3.6m/description.php) and our processed annotation from [Baidu Disk](https://pan.baidu.com/s/1Qg4dH8PBXm8SzApI-uu0GA) (code: kfsm) or [Google Drive](https://drive.google.com/file/d/1wZynXUq91yECVRTFV8Tetvo271BXzxwI/view?usp=sharing)\n2. Download MPII image from [MPII Human Pose Dataset](http://human-pose.mpi-inf.mpg.de/)\n3. Download COCO2017 image from [COCO Dataset](http://cocodataset.org/#home)\n4. Download cache file from [Dropbox](https://www.dropbox.com/sh/uouev0a1ao84ofd/AADAjJUdr_Fm-eubk7c_s2JTa?dl=0)\n5. Organize data like this\nCODE_BLOCK\n\n#:#: Usage\nWe have placed some example config files in *experiments* folder, and you can use them straight forward. Don't modify them unless you know exactly what it means.\n#:#:#: Train \nFor [Integral Human Pose Regression](https://arxiv.org/abs/1711.08229), cd to *pytorch_projects/integral_human_pose* \n**Integral Regression**\nCODE_BLOCK\n**Direct Joint Regression**\nCODE_BLOCK\n\nFor [3D pose estimation system](https://arxiv.org/abs/1809.06079) of ECCV18 Challenge, cd to *pytorch_projects/hm36_challenge*\nCODE_BLOCK\n\nBy default, logging and model will be saved to *log* and *output* folder respectively.\n\n#:#:#: Test\nTo run evaluation on CHALL_H80K Val dataset\n1. Download [model](https://www.dropbox.com/s/hfz5nkd39uisvbr/model_chall_train_152ft_384x288.pth.tar?dl=0)\n2. Place it under $project_root/model/hm36_challenge\n3. cd to *$project_root/pytorch_projects/hm36_challenge*\n4. execute command below\nCODE_BLOCK\n"}
{"id": 78, "title": "node-qa-masker-README.md", "text": "CODE_BLOCK\n\n"}
{"id": 79, "title": "GANimation-README.md", "text": "The code requires a directory containing the following files:\n- `imgs/`: folder with all image\n- `aus_openface.pkl`: dictionary containing the images action units.\n- `train_ids.csv`: file containing the images names to be used to train.\n- `test_ids.csv`: file containing the images names to be used to test.\n\nAn example of this directory is shown in `sample_dataset/`.\n\nTo generate the `aus_openface.pkl` extract each image Action Units with [OpenFace](https://github.com/TadasBaltrusaitis/OpenFace/wiki/Action-Units) and store each output in a csv file the same name as the image. Then run:\nCODE_BLOCK\n\n"}
{"id": 80, "title": "geojson-vt-README.md", "text": "Install using NPM (`npm install geojson-vt`) or Yarn (`yarn add geojson-vt`), then:\n\nCODE_BLOCK\n\nOr use a browser build directly:\n\nCODE_BLOCK\n"}
{"id": 81, "title": "tensorflow-README.md", "text": "To install the current release for CPU-only:\n\nCODE_BLOCK\n\nUse the GPU package for CUDA-enabled GPU cards:\n\nCODE_BLOCK\n\n*See [Installing TensorFlow](https://www.tensorflow.org/install) for detailed\ninstructions, and how to build from source.*\n\nPeople who are a little more adventurous can also try our nightly binaries:\n\n**Nightly pip packages** * We are pleased to announce that TensorFlow now offers\nnightly pip packages under the\n[tf-nightly](https://pypi.python.org/pypi/tf-nightly) and\n[tf-nightly-gpu](https://pypi.python.org/pypi/tf-nightly-gpu) project on PyPi.\nSimply run `pip install tf-nightly` or `pip install tf-nightly-gpu` in a clean\nenvironment to install the nightly TensorFlow build. We support CPU and GPU\npackages on Linux, Mac, and Windows.\n\n"}
{"id": 82, "title": "DeepGuidedFilter-README.md", "text": "[[Project]](http://wuhuikai.me/DeepGuidedFilterProject)    [[Paper]](http://wuhuikai.me/DeepGuidedFilterProject/deep_guided_filter.pdf)    [[arXiv]](https://arxiv.org/abs/1803.05619)    [[Demo]](http://wuhuikai.me/DeepGuidedFilterProject#demo)    [[Home]](http://wuhuikai.me)\n  \nOfficial implementation of **Fast End-to-End Trainable Guided Filter**.     \n**Faster**, **Better** and **Lighter**  for image processing and dense prediction. \n\n"}
{"id": 83, "title": "DeepGuidedFilter-README.md", "text": "1. Download source code from GitHub.\n    CODE_BLOCK\n2. Install dependencies (PyTorch version).\n    CODE_BLOCK\n3. (**Optional**) Install dependencies for MonoDepth (Tensorflow version).\n    CODE_BLOCK\n"}
{"id": 84, "title": "DeepGuidedFilter-README.md", "text": "* PyTorch Version\n    CODE_BLOCK\n* Tensorflow Version\n    CODE_BLOCK\n"}
{"id": 85, "title": "DeepGuidedFilter-README.md", "text": "CODE_BLOCK\n"}
{"id": 86, "title": "SRN-Deblur-README.md", "text": "Clone this project to your machine. \n\nCODE_BLOCK\n\n"}
{"id": 87, "title": "hyvr-README.md", "text": "Installing Python\n^^^^^^^^^^^^^^^^^\n\n\nWindows\n\"\"\"\"\"\"\"\n\nIf you are using Windows, we recommend installing the `Anaconda distribution\n`_ of Python 3. This distribution has the\nmajority of dependencies that HyVR requires.\n\nIt is also a good idea to install the HyVR package into a `virtual environment\n`_. Do this by\nopening a command prompt window and typing the following::\n\n    conda create --name hyvr_env\n\nYou need to then activate this environment::\n\n    conda activate hyvr_env\n\t\n\nLinux\n\"\"\"\"\"\n\nDepending on your preferences you can either use the Anaconda/Miniconda\ndistribution of python, or the version of your package manager. If you choose\nthe former, follow the same steps as for Windows.\n\nIf you choose the latter, you probably already have Python 3 installed. If not,\nyou can install it using your package manager (e.g. ``apt`` on Ubuntu/Debian).\n\nIn any way we recommend using a virtual environment. Non-conda users can use\n`virtualenvwrapper `_ or\n`pipenv `_.\n\n\nInstalling HyVR\n^^^^^^^^^^^^^^^\n\nOnce you have activated your virtual environment, you can install HyVR from PyPI using ``pip``::\n\n    pip install hyvr\n\nThe version on PyPI should always be up to date. If it's not, you can also\ninstall HyVR from github::\n\n    git clone https://github.com/driftingtides/hyvr.git\n    pip install hyvr\n\nTo install from source you need a C compiler.\n\nInstallation from conda-forge will (hopefully) be coming soon.\n\n\n"}
{"id": 88, "title": "striplog-README.md", "text": "    python setup.py sdist\n    pip install dist/striplog-0.6.1.tar.gz    "}
