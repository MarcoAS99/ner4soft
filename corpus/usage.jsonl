{"id": 1, "title": "Flow-Guided-Feature-Aggregation-README.md", "text": "1. To run the demo with our trained model (on ImageNet DET + VID train), please download the model manually from [OneDrive](https://1drv.ms/u/s!AqfHNsil2nOiiwDiKev7DB6L9ay7), and put it under folder `model/`.\n\n\tMake sure it looks like this:\n\t```\n\t./model/rfcn_fgfa_flownet_vid-0000.params\n\t```\n2. Run\n\t```\n\tpython ./fgfa_rfcn/demo.py\n\t```\n\n"}
{"id": 2, "title": "Flow-Guided-Feature-Aggregation-README.md", "text": "1. All of our experiment settings (GPU #, dataset, etc.) are kept in yaml config files at folder `./experiments/fgfa_rfcn/cfgs`.\n\n2. Two config files have been provided so far, namely, frame baseline (R-FCN) and the proposed FGFA  for ImageNet VID. We use 4 GPUs to train models on ImageNet VID.\n\n3. To perform experiments, run the python script with the corresponding config file as input. For example, to train and test FGFA with R-FCN, use the following command\n    ```\n    python experiments/fgfa_rfcn/fgfa_rfcn_end2end_train_test.py --cfg experiments/fgfa_rfcn/cfgs/resnet_v1_101_flownet_imagenet_vid_rfcn_end2end_ohem.yaml\n    ```\n\tA cache folder would be created automatically to save the model and the log under `output/fgfa_rfcn/imagenet_vid/`.\n\n4. Please find more details in config files and in our code.\n\n"}
{"id": 3, "title": "vue-devtools-README.md", "text": "1. If the page uses a production/minified build of Vue.js, devtools inspection is disabled by default so the Vue pane won't show up.\n\n2. To make it work for pages opened via `file://` protocol, you need to check \"Allow access to file URLs\" for this extension in Chrome's extension management panel.\n\n"}
{"id": 4, "title": "vue-devtools-README.md", "text": "```\n// Before you create app\nVue.config.devtools = process.env.NODE_ENV === 'development'\n\n// After you create app\nwindow.__VUE_DEVTOOLS_GLOBAL_HOOK__.Vue = app.constructor;\n\n// then had to add in ./store.js as well.\nVue.config.devtools = process.env.NODE_ENV === 'development'\n\n```\n\n"}
{"id": 5, "title": "LapSRN-README.md", "text": "    $ matlab\n    >> install\n   \nIf you install MatConvNet in your own path, you need to change the corresponding path in `install.m`, `train_LapSRN.m` and `test_LapSRN.m`.\n\n"}
{"id": 6, "title": "bootstrap-README.md", "text": "Several quick start options are available:\n\n- [Download the latest release.](https://github.com/twbs/bootstrap/archive/v4.3.1.zip)\n- Clone the repo: `git clone https://github.com/twbs/bootstrap.git`\n- Install with [npm](https://www.npmjs.com/): `npm install bootstrap`\n- Install with [yarn](https://yarnpkg.com/): `yarn add bootstrap@4.3.1`\n- Install with [Composer](https://getcomposer.org/): `composer require twbs/bootstrap:4.3.1`\n- Install with [NuGet](https://www.nuget.org/): CSS: `Install-Package bootstrap` Sass: `Install-Package bootstrap.sass`\n\nRead the [Getting started page](https://getbootstrap.com/docs/4.3/getting-started/introduction/) for information on the framework contents, templates and examples, and more.\n\n\n"}
{"id": 7, "title": "apsg-README.md", "text": "You can see APSG in action in accompanied Jupyter notebook [http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/apsg_tutorial.ipynb)\n\nAnd for fun check how simply you can animate stereonets\n[http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb](http://nbviewer.jupyter.org/github/ondrolexa/apsg/blob/master/examples/animation_example.ipynb)\n\n"}
{"id": 8, "title": "DCPDN-README.md", "text": "\tpython demo.py --dataroot ./facades/nat_new4 --valDataroot ./facades/nat_new4 --netG ./demo_model/netG_epoch_8.pth \u00a0 \nPre-trained dehazing model can be downloaded at (put it in the folder 'demo_model'): https://drive.google.com/drive/folders/1BmNP5ZUWEFeGGEL1NsZSRbYPyjBQ7-nn?usp=sharing\n\nTesting images (nature)  can be downloaded at (put it in the folder 'facades'):\nhttps://drive.google.com/drive/folders/1q5bRQGgS8SFEGqMwrLlku4Ad-0Tn3va7?usp=sharing\n\nTesting images (syn (Test A in the paper))  can be downloaded at (put it in the folder 'facades'):\nhttps://drive.google.com/drive/folders/1hbwYCzoI3R3o2Gj_kfT6GHG7RmYEOA-P?usp=sharing\n\n\n"}
{"id": 9, "title": "sentinelsat-README.md", "text": "  "}
{"id": 10, "title": "sentinelsat-README.md", "text": "  api.get_product_odata(, full=True)\n\nValid search query keywords can be found at the `Copernicus Open Access Hub documentation\n`_.\n\nCommand Line Interface\n----------------------\n\nA basic search query consists of a search area geometry as well as the username and\npassword to access the Copernicus Open Access Hub.\n\n.. code-block:: bash\n\n  sentinelsat -u  -p  -g \n\nSearch areas are provided as GeoJSON files, which can be created with\n`QGIS `_ or `geojson.io `_.\nIf you do not specify a start and end date only products published in the last\n24 hours will be queried.\n\nExample\n^^^^^^^\n\nSearch and download all Sentinel-1 scenes of type SLC, in descending\norbit, for the year 2015.\n\n.. code-block:: bash\n\n  sentinelsat -u  -p  -g  -s 20150101 -e 20151231 -d \\\n  --producttype SLC -q \"orbitdirection=Descending\" \\\n  --url \"https://scihub.copernicus.eu/dhus\"\n\nUsername, password and DHuS URL can also be set via environment variables for convenience.\n\n.. code-block:: bash\n \n  "}
{"id": 11, "title": "Shapely-README.md", "text": "Here is the canonical example of building an approximately circular patch by\nbuffering a point.\n\n.. code-block:: pycon\n\n    >>> from shapely.geometry import Point\n    >>> patch = Point(0.0, 0.0).buffer(10.0)\n    >>> patch\n    \n    >>> patch.area\n    313.65484905459385\n\nSee the manual for comprehensive usage snippets and the dissolve.py and\nintersect.py examples.\n\n"}
{"id": 12, "title": "Fiona-README.md", "text": "        "}
{"id": 13, "title": "Fiona-README.md", "text": "                "}
{"id": 14, "title": "ICNet-README.md", "text": "1. Clone the repository recursively:\n\n   ```shell\n   git clone --recursive https://github.com/hszhao/ICNet.git\n   ```\n\n2. Build Caffe and matcaffe:\n\n   ```shell\n   cd $ICNET_ROOT/PSPNet\n   cp Makefile.config.example Makefile.config\n   vim Makefile.config\n   make -j8 && make matcaffe\n   cd ..\n   ```\n\n3. Evaluation mIoU:\n\n   - Evaluation code is in folder 'evaluation'.\n   - Download trained models and put them in folder 'evaluation/model':\n     - icnet_cityscapes_train_30k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCRXpXMnVIbXdfaW8) \n\n       (31M, md5: c7038630c4b6c869afaaadd811bdb539; train on trainset for 30k)\n\n     - icnet_cityscapes_trainval_90k.caffemodel: [GoogleDrive](https://drive.google.com/open?id=0BzaU285cX7TCTFVpZWJINi1Iblk) \n\n       (31M, md5: 4f4dd9eecd465dd8de7e4cf88ba5d5d5; train on trainvalset for 90k)\n   - Modify the related paths in 'eval_all.m':\n     - Mainly variables 'data_root' and 'eval_list', and your image list for evaluation should be similar to that in folder 'evaluation/samplelist' if you use this evaluation code structure. \n\n   ```shell\n   cd evaluation\n   vim eval_all.m\n   ```\n\n   - Run the evaluation scripts:\n\n   ```\n   ./run.sh\n   ```\n\n4. Evaluation time:\n\n   - To get inference time as accurate as possible, it's suggested to make sure the GPU card with specified ID in script 'test_time.sh' is empty (without other processes executing)\n\n   - Run the evaluation scripts:\n\n   ```\n   ./test_time.sh\n   ```\n\n5. Results: \n\n   - Prediction results will show in folder 'evaluation/mc_result' and the expected scores are:\n     - ICNet train on trainset for 30K, evaluated on valset (mIoU/pAcc): 67.7/94.5\n     - ICNet train on trainvalset for 90K, evaluated on testset (mIoU): 69.5\n   - Log information of inference time will be in file 'time.log', approximately 33~36ms on TitanX.\n\n6. Demo video:\n\n   - Video processed by ICNet on cityscapes dataset:\n     - Alpha blending with value as 0.5: [Video](https://youtu.be/qWl9idsCuLQ)\n\n"}
{"id": 15, "title": "neural_renderer-README.md", "text": "```\npython ./examples/example1.py\npython ./examples/example2.py\npython ./examples/example3.py\npython ./examples/example4.py\n```\n\n\n"}
{"id": 16, "title": "neural_renderer-README.md", "text": "![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example1.gif)\n\n"}
{"id": 17, "title": "neural_renderer-README.md", "text": "Transforming the silhouette of a teapot into a rectangle. The loss function is the difference between the rendered image and the reference image.\n\nReference image, optimization, and the result.\n\n![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_optimization.gif) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example2_result.gif)\n\n"}
{"id": 18, "title": "neural_renderer-README.md", "text": "Matching the color of a teapot with a reference image.\n\nReference image, result.\n\n![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example3_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example3_result.gif)\n\n"}
{"id": 19, "title": "neural_renderer-README.md", "text": "The derivative of images with respect to camera pose can be computed through this renderer. In this example the position of the camera is optimized by gradient descent.\n\nFrom left to right: reference image, initial state, and optimization process.\n\n![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_ref.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_init.png) ![](https://raw.githubusercontent.com/hiroharu-kato/neural_renderer/master/examples/data/example4_result.gif)\n\n"}
{"id": 20, "title": "geonotebook-README.md", "text": "First provision the geoserver\n\n```\ncd devops/geoserver/\nvagrant up\n```\n\nSecond change the ```vis_server``` configuration to ```geoserver``` in the ```[default]``` section of your configuration. Then include a ```[geoserver]``` section with the pertinent configuration.  E.g.:\n\n```\n[default]\nvis_server=geoserver\n\n...\n\n[geoserver]\nusername = admin\npassword = geoserver\nurl = http://127.0.0.1:8080/geoserver\n```\n"}
{"id": 21, "title": "gprMax-README.md", "text": "We recommend using Miniconda to install Python and the required Python packages for gprMax in a self-contained Python environment. Miniconda is a mini version of Anaconda which is a completely free Python distribution (including for commercial use and redistribution). It includes more than 300 of the most popular Python packages for science, math, engineering, and data analysis.\n\n* `Download and install Miniconda `_. Choose the Python 3.x version for your platform. We recommend choosing the installation options to: install Miniconda only for your user account; add Miniconda to your PATH environment variable; and to register Miniconda Python as your default Python. See the `Quick Install page `_ for help installing Miniconda.\n* Open a Terminal (Linux/macOS) or Command Prompt (Windows) and run the following commands:\n\n.. code-block:: bash\n\n    $ conda update conda\n    $ conda install git\n    $ git clone https://github.com/gprMax/gprMax.git\n    $ cd gprMax\n    $ conda env create -f conda_env.yml\n\nThis will make sure conda is up-to-date, install Git, get the latest gprMax source code from GitHub, and create an environment for gprMax with all the necessary Python packages.\n\nIf you prefer to install Python and the required Python packages manually, i.e. without using Anaconda/Miniconda, look in the ``conda_env.yml`` file for a list of the requirements.\n\n"}
{"id": 22, "title": "kosmtik-README.md", "text": "To get command line help, run:\n\n    kosmtik -h\n\nTo run a Carto project (or `.yml`, `.yaml`):\n\n    kosmtik serve \n\nThen open your browser at http://127.0.0.1:6789/.\n\n\nYou may also want to install plugins. To see the list of available ones, type:\n\n    kosmtik plugins --available\n\nAnd then pick one and install it like this:\n\n    kosmtik plugins --install pluginname\n\nFor example:\n\n    kosmtik plugins --install kosmtik-map-compare [--install kosmtik-overlay\u2026]\n\n\n"}
{"id": 23, "title": "kosmtik-README.md", "text": "```json\n[\n    {\n        \"where\": \"center\",\n        \"then\": [-122.25, 49.25, 10]\n    },\n    {\n        \"where\": \"Layer\",\n        \"if\": {\n            \"Datasource.type\": \"postgis\"\n        },\n        \"then\": {\n            \"Datasource.dbname\": \"vancouver\",\n            \"Datasource.password\": \"\",\n            \"Datasource.user\": \"ybon\",\n            \"Datasource.host\": \"\"\n        }\n    },\n    {\n        \"where\": \"Layer\",\n        \"if\": {\n            \"id\": \"hillshade\"\n        },\n        \"then\": {\n            \"Datasource.file\": \"/home/ybon/Code/maps/hdm/DEM/data/hillshade.vrt\"\n        }\n    }\n]\n```\n\n"}
{"id": 24, "title": "kosmtik-README.md", "text": "```javascript\nexports.LocalConfig = function (localizer, project) {\n    localizer.where('center').then([29.9377, -3.4216, 9]);\n    localizer.where('Layer').if({'Datasource.type': 'postgis'}).then({\n        \"Datasource.dbname\": \"burundi\",\n        \"Datasource.password\": \"\",\n        \"Datasource.user\": \"ybon\",\n        \"Datasource.host\": \"\"\n    });\n    // You can also do it in pure JS\n    project.mml.bounds = [1, 2, 3, 4];\n};\n\n```\n\n"}
{"id": 25, "title": "two-stream-dyntex-synth-README.md", "text": "```\npython synthesize.py --type=dts --gpu=0 --runid=\"my_cool_fish\" --dynamics_target=data/dynamic_textures/fish --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel\n```\n\n"}
{"id": 26, "title": "two-stream-dyntex-synth-README.md", "text": "```\npython synthesize.py --type=dst --gpu=0 --runid=\"whoa_water!\" --dynamics_target=data/dynamic_textures/water_4 --appearance_target=data/textures/water_paint_cropped.jpeg --dynamics_model=models/MSOEnet_ucf101train01_6e-4_allaug_exceptscale_randorder.tfmodel\n```\n\n"}
{"id": 27, "title": "PRM-README.md", "text": "1. Install Nest's build-in Pytorch modules:\n\n    > To increase reusability, I abstracted some features from the original code, such as network trainer, to build Nest's built-in pytorch module set.\n    \n    ```bash\n    $ nest module install github@ZhouYanzhao/Nest:pytorch pytorch\n    ```\n\n2. Download the PASCAL-VOC2012 dataset:\n\n    ```bash\n    mkdir ./PRM/demo/datasets\n    cd ./PRM/demo/datasets\n    #: download and extract data\n    wget http://host.robots.ox.ac.uk/pascal/VOC/voc2012/VOCtrainval_11-May-2012.tar\n    tar xvf VOCtrainval_11-May-2012.tar\n    ```\n\n3. Run the demo experiment via [demo/main.ipynb](https://github.com/ZhouYanzhao/PRM/tree/pytorch/demo/main.ipynb)\n\n    ![PRM Segmentation](samples.png)\n\n"}
{"id": 28, "title": "tippecanoe-README.md", "text": "```\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_0_countries.zip\nunzip ne_10m_admin_0_countries.zip\nogr2ogr -f GeoJSON ne_10m_admin_0_countries.geojson ne_10m_admin_0_countries.shp\n\ncurl -L -O https://www.naturalearthdata.com/http//www.naturalearthdata.com/download/10m/cultural/ne_10m_admin_1_states_provinces.zip\nunzip -o ne_10m_admin_1_states_provinces.zip\nogr2ogr -f GeoJSON ne_10m_admin_1_states_provinces.geojson ne_10m_admin_1_states_provinces.shp\n\ntippecanoe -z3 -o countries-z3.mbtiles --coalesce-densest-as-needed ne_10m_admin_0_countries.geojson\ntippecanoe -zg -Z4 -o states-Z4.mbtiles --coalesce-densest-as-needed --extend-zooms-if-still-dropping ne_10m_admin_1_states_provinces.geojson\ntile-join -o states-countries.mbtiles countries-z3.mbtiles states-Z4.mbtiles\n```\n\nCountries:\n\n* `-z3`: Only generate zoom levels 0 through 3\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n\nStates and Provinces:\n\n* `-Z4`: Only generate zoom levels 4 and beyond\n* `-zg`: Automatically choose a maxzoom that should be sufficient to clearly distinguish the features and the detail within each feature\n* `--coalesce-densest-as-needed`: If the tiles are too big at low or medium zoom levels, merge as many features together as are necessary to allow tiles to be created with those features that are still distinguished\n* `--extend-zooms-if-still-dropping`: If even the tiles at high zoom levels are too big, keep adding zoom levels until one is reached that can represent all the features\n\n"}
{"id": 29, "title": "tippecanoe-README.md", "text": " * Make a tileset of the Natural Earth countries to zoom level 5, and also copy the GeoJSON features\n   to files in a `tiles/z/x/y.geojson` directory hierarchy.\n\n```\ntippecanoe -o countries.mbtiles -z5 -C 'mkdir -p tiles/$1/$2; tee tiles/$1/$2/$3.geojson' ne_10m_admin_0_countries.json\n```\n\n * Make a tileset of the Natural Earth countries to zoom level 5, but including only those tiles that\n   intersect the [bounding box of Germany](https://www.flickr.com/places/info/23424829).\n   (The `limit-tiles-to-bbox` script is [in the Tippecanoe source directory](filters/limit-tiles-to-bbox).)\n\n```\ntippecanoe -o countries.mbtiles -z5 -C './filters/limit-tiles-to-bbox 5.8662 47.2702 15.0421 55.0581 $*' ne_10m_admin_0_countries.json\n```\n\n * Make a tileset of TIGER roads in Tippecanoe County, leaving out all but primary and secondary roads (as [classified by TIGER](https://www.census.gov/geo/reference/mtfcc.html)) below zoom level 11.\n\n```\ntippecanoe -o roads.mbtiles -c 'if [ $1 -lt 11 ]; then grep \"\\\"MTFCC\\\": \\\"S1[12]00\\\"\"; else cat; fi' tl_2016_18157_roads.json\n```\n\nEnvironment\n-----------\n\nTippecanoe ordinarily uses as many parallel threads as the operating system claims that CPUs are available.\nYou can override this number by setting the `TIPPECANOE_MAX_THREADS` environmental variable.\n\nGeoJSON extension\n-----------------\n\nTippecanoe defines a GeoJSON extension that you can use to specify the minimum and/or maximum zoom level\nat which an individual feature will be included in the vector tileset being produced.\nIf you have a feature like this:\n\n```\n{\n    \"type\" : \"Feature\",\n    \"tippecanoe\" : { \"maxzoom\" : 9, \"minzoom\" : 4 },\n    \"properties\" : { \"FULLNAME\" : \"N Vasco Rd\" },\n    \"geometry\" : {\n        \"type\" : \"LineString\",\n        \"coordinates\" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]\n    }\n}\n```\n\nwith a `tippecanoe` object specifiying a `maxzoom` of 9 and a `minzoom` of 4, the feature\nwill only appear in the vector tiles for zoom levels 4 through 9. Note that the `tippecanoe`\nobject belongs to the Feature, not to its `properties`. If you specify a `minzoom` for a feature,\nit will be preserved down to that zoom level even if dot-dropping with `-r` would otherwise have\ndropped it.\n\nYou can also specify a layer name in the `tippecanoe` object, which will take precedence over\nthe filename or name specified using `--layer`, like this:\n\n```\n{\n    \"type\" : \"Feature\",\n    \"tippecanoe\" : { \"layer\" : \"streets\" },\n    \"properties\" : { \"FULLNAME\" : \"N Vasco Rd\" },\n    \"geometry\" : {\n        \"type\" : \"LineString\",\n        \"coordinates\" : [ [ -121.733350, 37.767671 ], [ -121.733600, 37.767483 ], [ -121.733131, 37.766952 ] ]\n    }\n}\n```\n\nIf your source GeoJSON only has `minzoom`, `maxzoom` and/or `layer` within `properties` you can use [ndjson-cli](https://github.com/mbostock/ndjson-cli/blob/master/README.md) to move them into the required `tippecanoe` object by piping the GeoJSON like this:\n\n```sh\nndjson-map 'd.tippecanoe = { minzoom: d.properties.minzoom, maxzoom: d.properties.maxzoom, layer: d.properties.layer }, delete d.properties.minzoom, delete d.properties.maxzoom, delete d.properties.layer, d'\n```\n\nGeometric simplifications\n-------------------------\n\nAt every zoom level, line and polygon features are subjected to Douglas-Peucker\nsimplification to the resolution of the tile.\n\nFor point features, it drops 1/2.5 of the dots for each zoom level above the\npoint base zoom (which is normally the same as the `-z` max zoom, but can be\na different zoom specified with `-B` if you have precise but sparse data).\nI don't know why 2.5 is the appropriate number, but the densities of many different\ndata sets fall off at about this same rate. You can use -r to specify a different rate.\n\nYou can use the gamma option to thin out especially dense clusters of points.\nFor any area where dots are closer than one pixel together (at whatever zoom level),\na gamma of 3, for example, will reduce these clusters to the cube root of their original density.\n\nFor line features, it drops any features that are too small to draw at all.\nThis still leaves the lower zooms too dark (and too dense for the 500K tile limit,\nin some places), so I need to figure out an equitable way to throw features away.\n\nUnless you specify `--no-tiny-polygon-reduction`,\nany polygons that are smaller than a minimum area (currently 4 square subpixels) will\nhave their probability diffused, so that some of them will be drawn as a square of\nthis minimum size and others will not be drawn at all, preserving the total area that\nall of them should have had together.\n\nFeatures in the same tile that share the same type and attributes are coalesced\ntogether into a single geometry if you use `--coalesce`. You are strongly encouraged to use -x to exclude\nany unnecessary attributes to reduce wasted file size.\n\nIf a tile is larger than 500K, it will try encoding that tile at progressively\nlower resolutions before failing if it still doesn't fit.\n\nDevelopment\n-----------\n\nRequires sqlite3 and zlib (should already be installed on MacOS). Rebuilding the manpage\nuses md2man (`gem install md2man`).\n\nLinux:\n\n    sudo apt-get install build-essential libsqlite3-dev zlib1g-dev\n\nThen build:\n\n    make\n\nand perhaps\n\n    make install\n\nTippecanoe now requires features from the 2011 C++ standard. If your compiler is older than\nthat, you will need to install a newer one. On MacOS, updating to the lastest XCode should\nget you a new enough version of `clang++`. On Linux, you should be able to upgrade `g++` with\n\n```\nsudo add-apt-repository -y ppa:ubuntu-toolchain-r/test\nsudo apt-get update -y\nsudo apt-get install -y g++-5\nexport CXX=g++-5\n```\n\nDocker Image\n------------\n\nA tippecanoe Docker image can be built from source and executed as a task to\nautomatically install dependencies and allow tippecanoe to run on any system\nsupported by Docker.\n\n```docker\n$ docker build -t tippecanoe:latest .\n$ docker run -it --rm \\\n  -v /tiledata:/data \\\n  tippecanoe:latest \\\n  tippecanoe --output=/data/output.mbtiles /data/example.geojson\n```\n\nThe commands above will build a Docker image from the source and compile the\nlatest version. The image supports all tippecanoe flags and options.\n\nExamples\n------\n\nCheck out [some examples of maps made with tippecanoe](MADE_WITH.md)\n\nName\n----\n\nThe name is [a joking reference](http://en.wikipedia.org/wiki/Tippecanoe_and_Tyler_Too) to a \"tiler\" for making map tiles.\n\ntile-join\n=========\n\nTile-join is a tool for copying and merging vector mbtiles files and for\njoining new attributes from a CSV file to existing features in them.\n\nIt reads the tiles from an\nexisting .mbtiles file or a directory of tiles, matches them against the\nrecords of the CSV (if one is specified), and writes out a new tileset.\n\nIf you specify multiple source mbtiles files or source directories of tiles,\nall the sources are read and their combined contents are written to the new\nmbtiles output. If they define the same layers or the same tiles, the layers\nor tiles are merged.\n\nThe options are:\n\n"}
{"id": 30, "title": "tippecanoe-README.md", "text": "Join Census LEHD ([Longitudinal Employer-Household Dynamics](https://lehd.ces.census.gov/)) employment data to a file of Census block geography\nfor Tippecanoe County, Indiana.\n\nDownload Census block geometry, and convert to GeoJSON:\n\n```\n$ curl -L -O https://www2.census.gov/geo/tiger/TIGER2010/TABBLOCK/2010/tl_2010_18157_tabblock10.zip\n$ unzip tl_2010_18157_tabblock10.zip\n$ ogr2ogr -f GeoJSON tl_2010_18157_tabblock10.json tl_2010_18157_tabblock10.shp\n```\n\nDownload Indiana employment data, and fix name of join key in header\n\n```\n$ curl -L -O https://lehd.ces.census.gov/data/lodes/LODES7/in/wac/in_wac_S000_JT00_2015.csv.gz\n$ gzip -dc in_wac_S000_JT00_2015.csv.gz | sed '1s/w_geocode/GEOID10/' > in_wac_S000_JT00_2015.csv\n```\n\nSort GeoJSON block geometry so it is ordered by block ID. If you don't do this, you will get a\n\"GeoJSON file is out of sort\" error.\n\n```\n$ tippecanoe-json-tool -e GEOID10 tl_2010_18157_tabblock10.json | LC_ALL=C sort > tl_2010_18157_tabblock10.sort.json\n```\n\nJoin block geometries to employment attributes:\n\n```\n$ tippecanoe-json-tool -c in_wac_S000_JT00_2015.csv tl_2010_18157_tabblock10.sort.json > blocks-wac.json\n```\n"}
{"id": 31, "title": "mapshaper-README.md", "text": "```\n\n"}
{"id": 32, "title": "pysal-README.md", "text": "If you are interested in contributing to PySAL please see our\n[development guidelines](https://github.com/pysal/pysal/wiki).\n\n"}
{"id": 33, "title": "mplstereonet-README.md", "text": "In most cases, you'll want to ``import mplstereonet`` and then make an axes\nwith ``projection=\"stereonet\"`` (By default, this is an equal-area stereonet).\nAlternately, you can use ``mplstereonet.subplots``, which functions identically\nto ``matplotlib.pyplot.subplots``, but creates stereonet axes.\n\nAs an example::\n\n    import matplotlib.pyplot as plt\n    import mplstereonet\n\n    fig = plt.figure()\n    ax = fig.add_subplot(111, projection='stereonet')\n\n    strike, dip = 315, 30\n    ax.plane(strike, dip, 'g-', linewidth=2)\n    ax.pole(strike, dip, 'g^', markersize=18)\n    ax.rake(strike, dip, -25)\n    ax.grid()\n\n    plt.show()\n\n.. image:: http://joferkington.github.com/mplstereonet/images/basic.png\n    :alt: A basic stereonet with a plane, pole to the plane, and rake along the plane\n    :align: center\n    :target: https://github.com/joferkington/mplstereonet/blob/master/examples/basic.py\n    \nPlanes, lines, poles, and rakes can be plotted using axes methods (e.g.\n``ax.line(plunge, bearing)`` or ``ax.rake(strike, dip, rake_angle)``).\n\nAll planar measurements are expected to follow the right-hand-rule to indicate\ndip direction. As an example, 315/30S would be 135/30 following the right-hand\nrule.\n\n"}
{"id": 34, "title": "gdal-docker-README.md", "text": "Running the container without any arguments will by default output the GDAL\nversion string as well as the supported raster and vector formats:\n\n    docker run geodata/gdal\n\nThe following command will open a bash shell in an Ubuntu based environment\nwith GDAL available:\n\n    docker run -t -i geodata/gdal /bin/bash\n\nYou will most likely want to work with data on the host system from within the\ndocker container, in which case run the container with the -v option. Assuming\nyou have a raster called `test.tif` in your current working directory on your\nhost system, running the following command should invoke `gdalinfo` on\n`test.tif`:\n\n    docker run -v $(pwd):/data geodata/gdal gdalinfo test.tif\n\nThis works because the current working directory is set to `/data` in the\ncontainer, and you have mapped the current working directory on your host to\n`/data`.\n\nNote that the image tagged `latest`, GDAL represents the latest code *at the\ntime the image was built*. If you want to include the most up-to-date commits\nthen you need to build the docker image yourself locally along these lines:\n\n    docker build -t geodata/gdal:local git://github.com/geo-data/gdal-docker/\n"}
{"id": 35, "title": "RDN-README.md", "text": "1. (optional) Download models for our paper and place them in '/RDN_TrainCode/experiment/model'.\n\n    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).\n\n2. Cd to 'RDN_TrainCode/code', run the following scripts to train models.\n\n    **You can use scripts in file 'TrainRDN_scripts' to train models for our paper.**\n\n    ```bash\n    #: BI, scale 2, 3, 4\n    #: BIX2F64D18C6G64P48, input=48x48, output=96x96\n    th main.lua -scale 2 -netType RDN -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true\n\n    #: BIX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX2.t7\n    th main.lua -scale 3 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true  -preTrained ../experiment/model/RDN_BIX2.t7\n\n    #: BIX4F64D18C6G64P32, input=32x32, output=128x128, fine-tune on RDN_BIX2.t7\n    th main.lua -scale 4 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 128 -dataset div2k -datatype t7  -DownKernel BI -splitBatch 4 -trainOnly true -nEpochs 1000 -preTrained ../experiment/model/RDN_BIX2.t7 \n\n    #: BD, scale 3\n    #: BDX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7\n    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel BD -splitBatch 4 -trainOnly true -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7\n\n    #: DN, scale 3\n    #: DNX3F64D18C6G64P32, input=32x32, output=96x96, fine-tune on RDN_BIX3.t7\n    th main.lua -scale 3 -nGPU 1 -netType resnet_cu -nFeat 64 -nFeaSDB 64 -nDenseBlock 16 -nDenseConv 8 -growthRate 64 -patchSize 96 -dataset div2k -datatype t7  -DownKernel DN -splitBatch 4 -trainOnly true  -nEpochs 200 -preTrained ../experiment/model/RDN_BIX3.t7\n    ```\n    Only RDN_BIX2.t7 was trained using 48x48 input patches. All other models were trained using 32x32 input patches in order to save training time.\n    However, smaller input patch size in training would lower the performance to some degree. We also set '-trainOnly true' to save GPU memory.\n"}
{"id": 36, "title": "RDN-README.md", "text": "1. Download models for our paper and place them in '/RDN_TestCode/model'.\n\n    All the models can be downloaded from [Dropbox](https://www.dropbox.com/sh/ngcvqdas167gol2/AAAdJe9w6s2fpo_KEGZe7d4Ra?dl=0) or [Baidu](https://pan.baidu.com/s/116FAzKnaJnAdxY_B6ENp_A).\n\n2. Run 'TestRDN.lua'\n\n    **You can use scripts in file 'TestRDN_scripts' to produce results for our paper.**\n\n    ```bash\n    #: No self-ensemble: RDN\n    #: BI degradation model, X2, X3, X4\n    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble false -dataset Set5\n    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble false -dataset Set5\n    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble false -dataset Set5\n    #: BD degradation model, X3\n    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble false -dataset Set5\n    #: DN degradation model, X3\n    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble false -dataset Set5\n\n\n    #: With self-ensemble: RDN+\n    #: BI degradation model, X2, X3, X4\n    th TestRDN.lua -model RDN_BIX2 -degradation BI -scale 2 -selfEnsemble true -dataset Set5\n    th TestRDN.lua -model RDN_BIX3 -degradation BI -scale 3 -selfEnsemble true -dataset Set5\n    th TestRDN.lua -model RDN_BIX4 -degradation BI -scale 4 -selfEnsemble true -dataset Set5\n    #: BD degradation model, X3\n    th TestRDN.lua -model RDN_BDX3 -degradation BD -scale 3 -selfEnsemble true -dataset Set5\n    #: DN degradation model, X3\n    th TestRDN.lua -model RDN_DNX3 -degradation DN -scale 3 -selfEnsemble true -dataset Set5\n    ```\n\n"}
{"id": 37, "title": "pyansys-README.md", "text": "    "}
{"id": 38, "title": "pyansys-README.md", "text": "    k += sparse.diags(np.random.random(k.shape[0])/1E20, shape=k.shape)\n\n    "}
{"id": 39, "title": "rasterio-README.md", "text": "    "}
{"id": 40, "title": "tilelive-mapnik-README.md", "text": "```javascript\nvar tilelive = require('tilelive');\nrequire('tilelive-mapnik').registerProtocols(tilelive);\n\ntilelive.load('mapnik:///path/to/file.xml', function(err, source) {\n    if (err) throw err;\n\n    // Interface is in XYZ/Google coordinates.\n    // Use `y = (1 << z) - 1 - y` to flip TMS coordinates.\n    source.getTile(0, 0, 0, function(err, tile, headers) {\n        // `err` is an error object when generation failed, otherwise null.\n        // `tile` contains the compressed image file as a Buffer\n        // `headers` is a hash with HTTP headers for the image.\n    });\n\n    // The `.getGrid` is implemented accordingly.\n});\n```\n\nNote that grid generation will only work when there's metadata inside a\n`` object in the Mapnik XML.\n\nThe key fields are `interactivity_layer` and `interactivity_fields`. See an\n[example in the tests](https://github.com/mapbox/tilelive-mapnik/blob/4e9cbf8347eba7c3c2b7e8fd4270ea39f9cc7af5/test/data/test.xml#L6-L7). These `Parameters` are normally added by the application that creates the XML,\nin this case [CartoCSS](https://github.com/mapbox/carto/blob/55fbafe0d0e8ec00515c5782a3664c15502f0437/lib/carto/renderer.js#L152-L189)\n"}
{"id": 41, "title": "map-vectorizer-README.md", "text": "![Example input map](https://raw.github.com/NYPL/map-vectorizer/master/example_input.png)\n\n"}
{"id": 42, "title": "map-vectorizer-README.md", "text": "![The resulting shapefile output superimposed](https://raw.github.com/NYPL/map-vectorizer/master/example_output.png)\n\n"}
{"id": 43, "title": "mplleaflet-README.md", "text": "The simplest use is to just create your plot using matplotlib commands and call `mplleaflet.show()`.\n\n```\n>>> import matplotlib.pyplot as plt\n... #: Load longitude, latitude data\n>>> plt.hold(True)\n#: Plot the data as a blue line with red squares on top\n#: Just plot longitude vs. latitude\n>>> plt.plot(longitude, latitude, 'b') #: Draw blue line\n>>> plt.plot(longitude, latitude, 'rs') #: Draw red squares\n```\n![matplotlib x,y plot](examples/images/simple_plot.png)\n\nNormally, displaying data as longitude, latitude will cause a cartographer to cry. That's totally fine with mplleaflet, Leaflet will project your data properly.\n\n```\n#: Convert to interactive Leaflet map\n>>> import mplleaflet\n>>> mplleaflet.show()\n```\n\n[Click to view final web page](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/readme_example.html)\n\n![Leaflet map preview](examples/images/simple_plot_map_preview.jpg)\n\nDisclaimer: Displaying data in spherical mercator might also cause a cartographer to cry.\n\n`show()` allows you to specify different tile layer URLs, CRS/EPSG codes, output files, etc. \n\n"}
{"id": 44, "title": "mplleaflet-README.md", "text": "* [basic_plot.py](examples/basic_plot.py): Simple line/point plotting. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/basic_plot.html).\n* [quiver.py](examples/quiver.py): Demonstrates use of quiver() to plot 2-D arrows. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/quiver.html).\n* [contour.py](examples/contour.py): Compute contour curves. This example demonstrates plotting in a different CRS and letting mplleaflet convert the output. [View the map](http://htmlpreview.github.io/?https://github.com/jwass/mplleaflet/master/examples/contour.html).\n* [Embedded IPython notebook example](http://nbviewer.ipython.org/github/jwass/mplleaflet/blob/master/examples/NYC%20Boroughs.ipynb)\n\n"}
{"id": 45, "title": "pose-residual-network-pytorch-README.md", "text": "We have tested our method on [Coco Dataset](http://cocodataset.org)\n\n"}
{"id": 46, "title": "react-README.md", "text": "We have several examples [on the website](https://reactjs.org/). Here is the first one to get you started:\n\n```jsx\nfunction HelloMessage({ name }) {\n  return Hello {name};\n}\n\nReactDOM.render(\n  ,\n  document.getElementById('container')\n);\n```\n\nThis example will render \"Hello Taylor\" into a container on the page.\n\nYou'll notice that we used an HTML-like syntax; [we call it JSX](https://reactjs.org/docs/introducing-jsx.html). JSX is not required to use React, but it makes code more readable, and writing it feels like writing HTML. If you're using React as a `` tag, read [this section](https://reactjs.org/docs/add-react-to-a-website.html#optional-try-react-with-jsx) on integrating JSX; otherwise, the [recommended JavaScript toolchains](https://reactjs.org/docs/create-a-new-react-app.html) handle it automatically.\n\n"}
{"id": 47, "title": "da-faster-rcnn-README.md", "text": "1. Build Caffe and pycaffe (see: [Caffe installation instructions](http://caffe.berkeleyvision.org/installation.html))\n\n2. Build the Cython modules\n    ```Shell\n    cd $FRCN_ROOT/lib\n    make\n    \n3. Follow the instrutions of [rbgirshick/py-faster-rcnn](https://github.com/rbgirshick/py-faster-rcnn) to download related data.\n    \n4. Prepare the dataset, source domain data should start with the filename 'source_', and target domain data with 'target_'.\n\n5. To train the Domain Adaptive Faster R-CNN:\n    ```Shell\n    cd $FRCN_ROOT\n    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  {NUM_ITER}  --cfg  {CONFIGURATION_FILE}\n    \n"}
{"id": 48, "title": "da-faster-rcnn-README.md", "text": "An example of adapting from **Cityscapes** dataset to **Foggy Cityscapes** dataset is provided:\n1. Download the datasets from [here](https://www.cityscapes-dataset.com/downloads/). Specifically, we will use **gtFine_trainvaltest.zip**, **leftImg8bit_trainvaltest.zip** and **leftImg8bit_trainvaltest_foggy.zip**.\n\n2. Prepare the data using the scripts in 'prepare_data/prepare_data.m'.\n\n3. Train the Domain Adaptive Faster R-CNN:\n    ```Shell\n    cd $FRCN_ROOT\n    ./tools/train_net.py --gpu {GPU_ID} --solver models/da_faster_rcnn/solver.prototxt --weights data/imagenet_models/VGG16.v2.caffemodel --imdb voc_2007_trainval --iters  70000  --cfg  models/da_faster_rcnn/faster_rcnn_end2end.yml\n    \n3. Test the trained model:\n    ```Shell\n    cd $FRCN_ROOT\n    ./tools/test_net.py --gpu {GPU_ID} --def models/da_faster_rcnn/test.prototxt --net output/faster_rcnn_end2end/voc_2007_trainval/vgg16_da_faster_rcnn_iter_70000.caffemodel --imdb voc_2007_test --cfg models/da_faster_rcnn/faster_rcnn_end2end.yml\n\n"}
{"id": 49, "title": "tensorflow-magenta-README.md", "text": "* [Installation](#installation)\n* [Using Magenta](#using-magenta)\n* [Playing a MIDI Instrument](#playing-a-midi-instrument)\n* [Development Environment (Advanced)](#development-environment)\n\n"}
{"id": 50, "title": "DID-MDN-README.md", "text": "\tpython test.py --dataroot ./facades/github --valDataroot ./facades/github --netG ./pre_trained/netG_epoch_9.pth \u00a0 \nPre-trained model can be downloaded at (put it in the folder 'pre_trained'): https://drive.google.com/drive/folders/1VRUkemynOwWH70bX9FXL4KMWa4s_PSg2?usp=sharing\n\nPre-trained density-aware model can be downloaded at (Put it in the folder 'classification'): https://drive.google.com/drive/folders/1-G86JTvv7o1iTyfB2YZAQTEHDtSlEUKk?usp=sharing\n\nPre-trained residule-aware model can be downloaded at (Put it in the folder 'residual_heavy'): https://drive.google.com/drive/folders/1bomrCJ66QVnh-WduLuGQhBC-aSWJxPmI?usp=sharing\n\n"}
{"id": 51, "title": "cltk-cltk-README.md", "text": "For interactive tutorials, in the form of Jupyter Notebooks, see .\n\n\n"}
{"id": 52, "title": "segyio-README.md", "text": "When segyio is built and installed, you're ready to start programming! Check\nout the [tutorial](#tutorial), [examples](#examples), [example\nprograms](python/examples), and [example\nnotebooks](https://github.com/equinor/segyio-notebooks). For a technical\nreference with examples and small recipes, [read the\ndocs](https://segyio.readthedocs.io/). API docs are also available with pydoc -\nstart your favourite Python interpreter and type `help(segyio)`, which should\nintegrate well with IDLE, pycharm and other Python tools.\n\n"}
{"id": 53, "title": "segyio-README.md", "text": "```python\nimport segyio\nimport numpy as np\nwith segyio.open('file.sgy') as f:\n    for trace in f.trace:\n        filtered = trace[np.where(trace < 1e-2)]\n```\n\nSee the [examples](#examples) for more.\n\n"}
{"id": 54, "title": "segyio-README.md", "text": "A copy of segyio is available both as pre-built binaries and source code:\n\n* In Debian [unstable](https://packages.debian.org/source/sid/segyio)\n    * `apt install python3-segyio`\n* Wheels for Python from [PyPI](https://pypi.python.org/pypi/segyio/)\n    * `pip install segyio`\n* Source code from [github](https://github.com/equinor/segyio)\n    * `git clone https://github.com/statoil/segyio`\n* Source code in [tarballs](https://github.com/equinor/segyio/releases)\n\n"}
{"id": 55, "title": "segyio-README.md", "text": "All code in this tutorial assumes segyio is imported, and that numpy is\navailable as np.\n\n```python\nimport segyio\nimport numpy as np\n```\n\nThis tutorial assumes you're familiar with Python and numpy. For a refresh,\ncheck out the [python tutorial](https://docs.python.org/3/tutorial/) and [numpy\nquickstart](https://docs.scipy.org/doc/numpy-dev/user/quickstart.html)\n\n"}
{"id": 56, "title": "segyio-README.md", "text": "```python\n>>> for line in f.iline[:2430]:\n...     print(np.average(line))\n\n>>> for line in f.xline[2:10]:\n...     print(line)\n\n>>> for line in f.fast[::2]:\n...     print(np.min(line))\n\n>>> for factor, offset in enumerate(f.iline[10, :]):\n...     offset *= factor\n        print(offset)\n\n>>> f.gather[200, 241, :].shape\n\n>>> text = f.text[0]\n>>> type(text)\n #: 'str' in Python 2\n\n>>> f.trace[10] = np.zeros(len(f.samples))\n```\n\nMore examples and recipes can be found in the docstrings `help(segyio)` and the\n[examples](#examples) section.\n\n"}
{"id": 57, "title": "ipyleaflet-README.md", "text": "**Selecting a basemap for a leaflet map:**\n\n![Basemap Screencast](basemap.gif)\n\n**Loading a geojson map:**\n\n![GeoJSON Screencast](geojson.gif)\n\n**Making use of leafletjs primitives:**\n\n![Primitives Screencast](primitives.gif)\n\n**Using the splitmap control:**\n\n![Splitmap Screencast](splitmap.gif)\n\n**Displaying velocity data on the top of a map:**\n\n![Velocity Screencast](velocity.gif)\n\n**Choropleth layer:**\n\n![Choropleth Screencast](choropleth.gif)\n\n"}
{"id": 58, "title": "PVGeo-README.md", "text": "To begin using the *PVGeo* Python package, create/activate your Python virtual\nenvironment (we highly recommend using anaconda) and install *PVGeo* through pip:\n\n```bash\npip install PVGeo\n```\n\nNow *PVGeo* is ready for use in your standard Python environment (2.7 or >=3.6)\nwith all dependencies installed! Go ahead and test your install:\n\n```bash\npython -c \"import PVGeo; print(PVGeo.__version__)\"\n```\n\nNote that Windows users must use Python >=3.6 when outside of ParaView.\nFurther insight can be found in the [**Getting Started Guide**](http://pvgeo.org/overview/getting-started.html).\n\n\n"}
{"id": 59, "title": "readgssi-README.md", "text": "To display the help text:\n\n```bash\n$ readgssi -h\n\nusage:\nreadgssi -i input.DZT [OPTIONS]\n\noptional flags:\n     OPTION     |      ARGUMENT       |       FUNCTIONALITY\n-o, --output    | file:  /dir/f.ext   |  specify an output file\n-f, --format    | string, eg. \"csv\"   |  specify output format (csv is the only working format currently)\n-p, --plot      | +integer or \"auto\"  |  plot will be x inches high (dpi=150), or \"auto\". default: 10\n-x, --xscale    | string, eg. \"dist\"  |  readgssi will attempt to convert the x-axis to distance, time, or traces based on header values\n-z, --zscale    | string, eg. \"time\"  |  readgssi will attempt to convert the x-axis to depth, time, or samples based on header values\n-n, --noshow    |                     |  suppress matplotlib popup window and simply save a figure (useful for multiple file processing)\n-c, --colormap  | string, eg. \"Greys\" |  specify the colormap (https://matplotlib.org/users/colormaps.html#:grayscale-conversion)\n-g, --gain      | positive (+)integer |  gain value (higher=greater contrast, default: 1)\n-r, --bgr       |                     |  horizontal background removal algorithm (useful to remove ringing)\n-R, --reverse   |                     |  reverse (flip radargram horizontally)\n-w, --dewow     |                     |  trinomial dewow algorithm\n-t, --bandpass  | +int-+int (MHz)     |  butterworth bandpass filter (positive integer range in megahertz; ex. 100-145)\n-b, --colorbar  |                     |  add a colorbar to the radar figure\n-a, --antfreq   | positive integer    |  specify antenna frequency (read automatically if not given)\n-s, --stack     | +integer or \"auto\"  |  specify trace stacking value or \"auto\" to autostack to ~2.5:1 x:y axis ratio\n-N, --normalize |                     |  reads a .DZG NMEA data if it exists; otherwise tries to read a csv file with lat, lon, and time fields to distance normalize with\n-d, --spm       | positive float      |  specify the samples per meter (spm) manually. overrides header value.\n-m, --histogram |                     |  produce a histogram of data values\n-E, --epsr      | float > 1.0         |  user-defined epsilon sub r (sometimes referred to as \"dielectric\"; ignores value in DZT header)\n-Z, --zero      | positive integer    |  skip this many samples from the top of the trace downward (useful for removing transceiver delay)\n\nnaming scheme for exports:\n   CHARACTERS   |      MEANING\n    c0          |  Profile from channel 0 (can range from 0 - 3)\n    Dn          |  Distance normalization\n    Tz233       |  Time zero at 233 samples\n    S8          |  Stacked 8 times\n    Rv          |  Profile read in reverse (flipped horizontally)\n    Bgr         |  Background removal filter\n    Dw          |  Dewow filter\n    Bp100-145   |  2-corner bandpass filter applied from 100 to 145 MHz\n    G30         |  30x contrast gain\n```\n\nFrom a unix command line:\n```bash\nreadgssi -i DZT__001.DZT\n```\nSimply specifying an input DZT file like in the above command (`-i file`) will display a host of data about the file including:\n- name of GSSI control unit\n- antenna model\n- antenna frequency\n- samples per trace\n- bits per sample\n- traces per second\n- L1 dielectric as entered during survey\n- sampling depth\n- speed of light at given dielectric\n- number of traces\n- number of seconds\n\n"}
{"id": 60, "title": "readgssi-README.md", "text": "```bash\nreadgssi -i DZT__001.DZT -p 5 -s auto -c viridis -m\n```\nThe above command will cause `readgssi` to save and show a plot named \"DZT__001_100MHz.png\" with a y-size of 6 inches at 150 dpi (`-p 6`) and the autostacking algorithm will stack the x-axis to some multiple of times shorter than the original data array for optimal viewing on a monitor, approximately 2.5\\*y (`-s auto`). The plot will be rendered in the viridis color scheme, which is the default for matplotlib. The `-m` flag will draw a histogram for each data channel.\n![Example 1a](https://github.com/iannesbitt/readgssi/raw/master/examples/1a.png)\n![Example 1a histogram](https://github.com/iannesbitt/readgssi/raw/master/examples/1a-h.png)\n\n"}
{"id": 61, "title": "readgssi-README.md", "text": "```bash\nreadgssi -i DZT__001.DZT -o 1b.png -p 5 -s auto -c viridis -g 50 -m -r -w\n```\nThis will cause `readgssi` to create a plot from the same file, but matplotlib will save the plot as \"1b.png\" (`-o 1b.png`). The script will plot the y-axis size (`-p 5`) and automatically stack the x-axis to (`-s auto`). The script will plot the data with a gain value of 50 (`-g 50`), which will increase the plot contrast by a factor of 50. Next `readgssi` will run the background removal (`-r`) and dewow (`-w`) filters. Finally, the `-m` flag will draw a histogram for each data channel. Note how the histogram changes when filters are applied.\n![Example 1b](https://github.com/iannesbitt/readgssi/raw/master/examples/1b.png)\n![Example 1b histogram](https://github.com/iannesbitt/readgssi/raw/master/examples/1b-h.png)\n\n"}
{"id": 62, "title": "readgssi-README.md", "text": "```bash\nreadgssi -i DZT__001.DZT -o 1c.png -p 5 -s auto -r -w -c seismic\n```\nHere, background removal and dewow filters are applied, but no gain adjustments are made (equivalent to `-g 1`). The script uses matplotlib's \"seismic\" colormap (`-c seismic`) which is specifically designed for this type of waterfall array plotting. Even without gain, you will often be able to easily see very slight signal perturbations. It is not colorblind-friendly for either of the two most common types of human colorblindness, however, which is why it is not the default colormap.\n![Example 1c](https://github.com/iannesbitt/readgssi/raw/master/examples/1c.png)\n\n"}
{"id": 63, "title": "readgssi-README.md", "text": "```bash\nreadgssi -i DZT__002.DZT -o 2a.png -p 10 -s 3 -n\n```\nHere `readgssi` will create a plot of size 10 and stack 3x (`-p 10 -s 3`). Matplotlib will use the default \"Greys\" colormap and save a PNG of the figure, but the script will suppress the matplotlib window (`-n`, useful for processing an entire directory full of DZTs at once).\n![Example 2a](https://github.com/iannesbitt/readgssi/raw/master/examples/2a.png)\n\n"}
{"id": 64, "title": "readgssi-README.md", "text": "```bash\nreadgssi -i DZT__002.DZT -o 2b.png -p 10 -s 3 -n -r\n```\nThe script does the same thing, except it applies horizontal mean background removal `-r`. Note the difference in ringing artifacts between examples 2a and 2b.\n![Example 2b](https://github.com/iannesbitt/readgssi/raw/master/examples/2b.png)\n\n\n"}
{"id": 65, "title": "RESCAN-README.md", "text": "    python train.py\n    python eval.py\n    python show.py\n\n"}
{"id": 66, "title": "Detectron-README.md", "text": "After installation, please see [`GETTING_STARTED.md`](GETTING_STARTED.md) for brief tutorials covering inference and training with Detectron.\n\n"}
{"id": 67, "title": "Detectron-README.md", "text": "To start, please check the [troubleshooting](INSTALL.md#troubleshooting) section of our installation instructions as well as our [FAQ](FAQ.md). If you couldn't find help there, try searching our GitHub issues. We intend the issues page to be a forum in which the community collectively troubleshoots problems.\n\nIf bugs are found, **we appreciate pull requests** (including adding Q&A's to `FAQ.md` and improving our installation instructions and troubleshooting documents). Please see [CONTRIBUTING.md](CONTRIBUTING.md) for more information about contributing to Detectron.\n\n"}
{"id": 68, "title": "facebookresearch-pyrobot-README.md", "text": "Please refer to [pyrobot.org](https://pyrobot.org/) and [locobot.org](http://locobot.org)\n\n"}
{"id": 69, "title": "DaSiamRPN-README.md", "text": "  \n\n\n- To reproduce the reuslts on paper, the pretrained model can be downloaded from [Google Drive](https://drive.google.com/open?id=1BtIkp5pB6aqePQGlMb2_Z7bfPy6XEj6H): `SiamRPNOTB.model`. \n:zap: :zap: This model is the **fastest** (~200fps) Siamese Tracker with AUC of 0.655 on OTB2015. :zap: :zap: \n\n- You must download OTB2015 dataset (download [script](code/data/get_otb_data.sh)) at first.\n\nA simple test example.\n\n```\ncd code\npython demo.py\n```\n\nIf you want to test the performance on OTB2015, please using the follwing command.\n\n```\ncd code\npython test_otb.py\npython eval_otb.py OTB2015 \"Siam*\" 0 1\n```\n\n\n"}
{"id": 70, "title": "hmr-README.md", "text": "https://github.com/mattloper/chumpy/tree/db6eaf8c93eb5ae571eb054575fb6ecec62fd86d\n\n\n"}
{"id": 71, "title": "hmr-README.md", "text": "1. Download the pre-trained models\n```\nwget https://people.eecs.berkeley.edu/~kanazawa/cachedir/hmr/models.tar.gz && tar -xf models.tar.gz\n```\n\n2. Run the demo\n```\npython -m demo --img_path data/coco1.png\npython -m demo --img_path data/im1954.jpg\n```\n\nImages should be tightly cropped, where the height of the person is roughly 150px.\nOn images that are not tightly cropped, you can run\n[openpose](https://github.com/CMU-Perceptual-Computing-Lab/openpose) and supply\nits output json (run it with `--write_json` option).\nWhen json_path is specified, the demo will compute the right scale and bbox center to run HMR:\n```\npython -m demo --img_path data/random.jpg --json_path data/random_keypoints.json\n```\n(The demo only runs on the most confident bounding box, see `src/util/openpose.py:get_bbox`)\n\n"}
{"id": 72, "title": "hmr-README.md", "text": "1. Download pre-trained models like above.\n2. Run webcam Demo\n2. Run the demo\n```\npython -m demo --img_path data/coco1.png\npython -m demo --img_path data/im1954.jpg\n```\n\n"}
{"id": 73, "title": "tetgen-README.md", "text": "    cells = grid.cells.reshape(-1, 5)[:, 1:]\n    cell_center = grid.points[cells].mean(1)\n\n    "}
{"id": 74, "title": "GAN_stability-README.md", "text": "First download your data and put it into the `./data` folder.\n\nTo train a new model, first create a config script similar to the ones provided in the `./configs` folder.  You can then train you model using\n```\npython train.py PATH_TO_CONFIG\n```\n\nTo compute the inception score for your model and generate samples, use\n```\npython test.py PATH_TO_CONIFG\n```\n\nFinally, you can create nice latent space interpolations using\n```\npython interpolate.py PATH_TO_CONFIG\n```\nor\n```\npython interpolate_class.py PATH_TO_CONFIG\n```\n\n"}
{"id": 75, "title": "pylops-README.md", "text": "You need **Python 3.6.4 or greater**.\n\n"}
{"id": 76, "title": "generator-arcgis-js-app-README.md", "text": "`grunt` - default task, will output code to a `dist` folder with sourcemaps.\n\n`grunt dev` - will start a local server on at `http://localhost:8282/` and watch for changes. Uses livereload to refresh browser with each update.\n\n`http://localhost:8282/dist/` - application\n\n`http://localhost:8282/node_modules/intern/client.html?config=tests/intern` - test suites\n\n`grunt build` - build the application and output to a `release` folder.\n\n`grunt e2e` - runs all tests using local [chromedriver](https://sites.google.com/a/chromium.org/chromedriver/).\n\n\n"}
{"id": 77, "title": "generator-arcgis-js-app-README.md", "text": "Yeoman has a heart of gold. He's a person with feelings and opinions, but he's very easy to work with. If you think he's too opinionated, he can be easily convinced.\n\nIf you'd like to get to know Yeoman better and meet some of his friends, [Grunt](http://gruntjs.com) and [Bower](http://bower.io), check out the complete [Getting Started Guide](https://github.com/yeoman/yeoman/wiki/Getting-Started).\n\n\n"}
{"id": 78, "title": "puppeteer-README.md", "text": "Note: Puppeteer requires at least Node v6.4.0, but the examples below use async/await which is only supported in Node v7.6.0 or greater.\n\nPuppeteer will be familiar to people using other browser testing frameworks. You create an instance\nof `Browser`, open pages, and then manipulate them with [Puppeteer's API](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#).\n\n**Example** - navigating to https://example.com and saving a screenshot as *example.png*:\n\nSave file as **example.js**\n\n```js\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  await page.goto('https://example.com');\n  await page.screenshot({path: 'example.png'});\n\n  await browser.close();\n})();\n```\n\nExecute script on the command line\n\n```bash\nnode example.js\n```\n\nPuppeteer sets an initial page size to 800px x 600px, which defines the screenshot size. The page size can be customized  with [`Page.setViewport()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagesetviewportviewport).\n\n**Example** - create a PDF.\n\nSave file as **hn.js**\n\n```js\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  await page.goto('https://news.ycombinator.com', {waitUntil: 'networkidle2'});\n  await page.pdf({path: 'hn.pdf', format: 'A4'});\n\n  await browser.close();\n})();\n```\n\nExecute script on the command line\n\n```bash\nnode hn.js\n```\n\nSee [`Page.pdf()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pagepdfoptions) for more information about creating pdfs.\n\n**Example** - evaluate script in the context of the page\n\nSave file as **get-dimensions.js**\n\n```js\nconst puppeteer = require('puppeteer');\n\n(async () => {\n  const browser = await puppeteer.launch();\n  const page = await browser.newPage();\n  await page.goto('https://example.com');\n\n  // Get the \"viewport\" of the page, as reported by the page.\n  const dimensions = await page.evaluate(() => {\n    return {\n      width: document.documentElement.clientWidth,\n      height: document.documentElement.clientHeight,\n      deviceScaleFactor: window.devicePixelRatio\n    };\n  });\n\n  console.log('Dimensions:', dimensions);\n\n  await browser.close();\n})();\n```\n\nExecute script on the command line\n\n```bash\nnode get-dimensions.js\n```\n\nSee [`Page.evaluate()`](https://github.com/GoogleChrome/puppeteer/blob/v1.18.0/docs/api.md#pageevaluatepagefunction-args) for more information on `evaluate` and related methods like `evaluateOnNewDocument` and `exposeFunction`.\n\n\n\n\n"}
{"id": 79, "title": "puppeteer-README.md", "text": "        env DEBUG=\"puppeteer:*\" env DEBUG_COLORS=true node script.js 2>&1 | grep -v '\"Network'\n\n7. Debug your Puppeteer (node) code easily, using [ndb](https://github.com/GoogleChromeLabs/ndb)\n\n  - `npm install -g ndb` (or even better, use [npx](https://github.com/zkat/npx)!)\n\n  - add a `debugger` to your Puppeteer (node) code\n\n  - add `ndb` (or `npx ndb`) before your test command. For example:\n\n    `ndb jest` or `ndb mocha` (or `npx ndb jest` / `npx ndb mocha`)\n\n  - debug your test inside chromium like a boss!\n\n\n\n\n"}
{"id": 80, "title": "puppeteer-README.md", "text": "Look for `chromium_revision` in [package.json](https://github.com/GoogleChrome/puppeteer/blob/master/package.json).\n\n"}
{"id": 81, "title": "geojson-vt-README.md", "text": "Here's **geojson-vt** action in [Mapbox GL JS](https://github.com/mapbox/mapbox-gl-js),\ndynamically loading a 100Mb US zip codes GeoJSON with 5.4 million points:\n\n![](https://cloud.githubusercontent.com/assets/25395/5360312/86028d8e-7f91-11e4-811f-87f24acb09ca.gif)\n\nThere's a convenient [debug page](http://mapbox.github.io/geojson-vt/debug/) to test out **geojson-vt** on different data.\nJust drag any GeoJSON on the page, watching the console.\n\n![](https://cloud.githubusercontent.com/assets/25395/5363235/41955c6e-7fa8-11e4-9575-a66ef54cb6d9.gif)\n\n"}
{"id": 82, "title": "geojson-vt-README.md", "text": "```js\n// build an initial index of tiles\nvar tileIndex = geojsonvt(geoJSON);\n\n// request a particular tile\nvar features = tileIndex.getTile(z, x, y).features;\n\n// show an array of tile coordinates created so far\nconsole.log(tileIndex.tileCoords); // [{z: 0, x: 0, y: 0}, ...]\n```\n\n"}
{"id": 83, "title": "DeepGuidedFilter-README.md", "text": "* PyTorch Version\n    ```python\n    from guided_filter_pytorch.guided_filter import FastGuidedFilter\n    \n    hr_y = FastGuidedFilter(r, eps)(lr_x, lr_y, hr_x)\n    ```\n    ```python\n    from guided_filter_pytorch.guided_filter import GuidedFilter\n    \n    hr_y = GuidedFilter(r, eps)(hr_x, init_hr_y)\n    ``` \n* Tensorflow Version\n    ```python\n    from guided_filter_tf.guided_filter import fast_guided_filter\n    \n    hr_y = fast_guided_filter(lr_x, lr_y, hr_x, r, eps, nhwc)\n    ```\n    ```python\n    from guided_filter_tf.guided_filter import guided_filter\n    \n    hr_y = guided_filter(hr_x, init_hr_y, r, eps, nhwc)\n    ```\n"}
{"id": 84, "title": "DeepGuidedFilter-README.md", "text": "* [Image Processing](ImageProcessing/DeepGuidedFilteringNetwork)\n* [Semantic Segmentation with Deeplab-Resnet](ComputerVision/Deeplab-Resnet)\n* [Saliency Detection with DSS](ComputerVision/Saliency_DSS)\n* [Monocular Depth Estimation (TensorFlow version)](ComputerVision/MonoDepth)\n\n"}
{"id": 85, "title": "hyvr-README.md", "text": "To use HyVR you have to create a configuration file with your settings.\nYou can then run HyVR the following way::\n\n    (hyvr_env) $ python -m hyvr my_configfile.ini\n\nHyVR will then run and store all results in a subdirectory. If no configfile is\ngiven, it will run a test case instead::\n\n    (hyvr_env) $ python -m hyvr\n\nIf you want to use HyVR in a script, you can import it and use the ``run`` function::\n\n    import hyvr\n    hyvr.run('my_configfile.ini')\n    \nExamples can be found in the ``testcases`` directory of the `github repository\n`_, the general setup and possible\noptions of the config-file are described in the documentation.\nCurrently only ``made.ini`` is ported to version 1.0.0.\n\n"}
{"id": 86, "title": "lasio-README.md", "text": "Install the usual way:\n\n```bash\n$ pip install lasio\n```\n\nVery quick example session:\n\n```python\n>>> import lasio\n>>> las = lasio.read(\"sample_big.las\")\n```\n\nData is accessible both directly as numpy arrays\n\n```python\n>>> las.keys()\n['DEPT', 'DT', 'RHOB', 'NPHI', 'SFLU', 'SFLA', 'ILM', 'ILD']\n>>> las['SFLU']\narray([ 123.45,  123.45,  123.45, ...,  123.45,  123.45,  123.45])\n>>> las['DEPT']\narray([ 1670.   ,  1669.875,  1669.75 , ...,  1669.75 ,  1670.   ,\n        1669.875])\n```\n\nand as ``CurveItem`` objects with associated metadata:\n\n```python\n>>> las.curves\n[CurveItem(mnemonic=DEPT, unit=M, value=, descr=1  DEPTH, original_mnemonic=DEPT, data.shape=(29897,)), \nCurveItem(mnemonic=DT, unit=US/M, value=, descr=2  SONIC TRANSIT TIME, original_mnemonic=DT, data.shape=(29897,)), \nCurveItem(mnemonic=RHOB, unit=K/M3, value=, descr=3  BULK DENSITY, original_mnemonic=RHOB, data.shape=(29897,)), \nCurveItem(mnemonic=NPHI, unit=V/V, value=, descr=4   NEUTRON POROSITY, original_mnemonic=NPHI, data.shape=(29897,)), \nCurveItem(mnemonic=SFLU, unit=OHMM, value=, descr=5  RXO RESISTIVITY, original_mnemonic=SFLU, data.shape=(29897,)), \nCurveItem(mnemonic=SFLA, unit=OHMM, value=, descr=6  SHALLOW RESISTIVITY, original_mnemonic=SFLA, data.shape=(29897,)), \nCurveItem(mnemonic=ILM, unit=OHMM, value=, descr=7  MEDIUM RESISTIVITY, original_mnemonic=ILM, data.shape=(29897,)), \nCurveItem(mnemonic=ILD, unit=OHMM, value=, descr=8  DEEP RESISTIVITY, original_mnemonic=ILD, data.shape=(29897,))]\n```\n\nHeader information is parsed into simple HeaderItem objects, and stored in a dictionary for each section of the header:\n\n```python\n>>> las.version\n[HeaderItem(mnemonic=VERS, unit=, value=1.2, descr=CWLS LOG ASCII STANDARD -VERSION 1.2, original_mnemonic=VERS), \nHeaderItem(mnemonic=WRAP, unit=, value=NO, descr=ONE LINE PER DEPTH STEP, original_mnemonic=WRAP)]\n>>> las.well\n[HeaderItem(mnemonic=STRT, unit=M, value=1670.0, descr=, original_mnemonic=STRT), \nHeaderItem(mnemonic=STOP, unit=M, value=1660.0, descr=, original_mnemonic=STOP), \nHeaderItem(mnemonic=STEP, unit=M, value=-0.125, descr=, original_mnemonic=STEP), \nHeaderItem(mnemonic=NULL, unit=, value=-999.25, descr=, original_mnemonic=NULL), \nHeaderItem(mnemonic=COMP, unit=, value=ANY OIL COMPANY LTD., descr=COMPANY, original_mnemonic=COMP), \nHeaderItem(mnemonic=WELL, unit=, value=ANY ET AL OIL WELL #:12, descr=WELL, original_mnemonic=WELL), \nHeaderItem(mnemonic=FLD, unit=, value=EDAM, descr=FIELD, original_mnemonic=FLD), \nHeaderItem(mnemonic=LOC, unit=, value=A9-16-49, descr=LOCATION, original_mnemonic=LOC), \nHeaderItem(mnemonic=PROV, unit=, value=SASKATCHEWAN, descr=PROVINCE, original_mnemonic=PROV), \nHeaderItem(mnemonic=SRVC, unit=, value=ANY LOGGING COMPANY LTD., descr=SERVICE COMPANY, original_mnemonic=SRVC), \nHeaderItem(mnemonic=DATE, unit=, value=25-DEC-1988, descr=LOG DATE, original_mnemonic=DATE), \nHeaderItem(mnemonic=UWI, unit=, value=100091604920, descr=UNIQUE WELL ID, original_mnemonic=UWI)]\n>>> las.params\n[HeaderItem(mnemonic=BHT, unit=DEGC, value=35.5, descr=BOTTOM HOLE TEMPERATURE, original_mnemonic=BHT), \nHeaderItem(mnemonic=BS, unit=MM, value=200.0, descr=BIT SIZE, original_mnemonic=BS), \nHeaderItem(mnemonic=FD, unit=K/M3, value=1000.0, descr=FLUID DENSITY, original_mnemonic=FD), \nHeaderItem(mnemonic=MATR, unit=, value=0.0, descr=NEUTRON MATRIX(0=LIME,1=SAND,2=DOLO), original_mnemonic=MATR), \nHeaderItem(mnemonic=MDEN, unit=, value=2710.0, descr=LOGGING MATRIX DENSITY, original_mnemonic=MDEN), \nHeaderItem(mnemonic=RMF, unit=OHMM, value=0.216, descr=MUD FILTRATE RESISTIVITY, original_mnemonic=RMF), \nHeaderItem(mnemonic=DFD, unit=K/M3, value=1525.0, descr=DRILL FLUID DENSITY, original_mnemonic=DFD)]\n```\n\nThe data is stored as a 2D numpy array:\n\n```python\n>>> las.data\narray([[ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       ...,\n       [ 1669.75 ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1670.   ,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ],\n       [ 1669.875,   123.45 ,  2550.   , ...,   123.45 ,   110.2  ,   105.6  ]])\n```\n\nYou can also retrieve and load data as a ``pandas`` DataFrame, build LAS files from scratch, \nwrite them back to disc, and export to Excel, amongst other things.\n\nSee the [documentation](https://lasio.readthedocs.io/en/latest/) for more details.\n\n"}
